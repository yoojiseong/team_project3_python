{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os  # CSV 저장을 위한 폴더/파일 관리를 위해 import\n",
    "# from selenium import webdriver\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "def clean_filename(filename):\n",
    "    \"\"\"파일 이름으로 사용할 수 없는 문자를 '_'로 대체합니다.\"\"\"\n",
    "    invalid_chars = '\\\\/*?:\"<>|().'\n",
    "    cleaned_name = filename.strip() # 양쪽 공백 제거\n",
    "    for char in invalid_chars:\n",
    "        cleaned_name = cleaned_name.replace(char, '_')\n",
    "    cleaned_name = cleaned_name.replace(' ', '_') # 공백도 '_'로 대체\n",
    "    # 혹시 모를 연속된 '_'를 하나로 변경 (선택적)\n",
    "    while \"__\" in cleaned_name:\n",
    "        cleaned_name = cleaned_name.replace(\"__\", \"_\")\n",
    "    return cleaned_name\n",
    "\n",
    "def scrape_full_preview(driver, wait, scroll_wait):\n",
    "    \"\"\"\n",
    "    [최종 수정됨] StaleElementReferenceException을 방지하기 위해\n",
    "    루프 내에서 tbody를 계속 갱신하고, 파싱 직전에도 갱신합니다.\n",
    "    \"\"\"\n",
    "    print(\"    [Sub] '데이터 미리보기' 탭의 그리드 로드 대기 중...\")\n",
    "    try:\n",
    "        # 그리드 헤더와 본문이 로드될 때까지 대기\n",
    "        wait.until(EC.visibility_of_element_located((By.ID, 'sheet1_Header')))\n",
    "        # 초기 tbody 확인 (이 변수를 루프에서 재사용하지 않음)\n",
    "        initial_tbody = wait.until(EC.visibility_of_element_located((By.ID, 'sheet1_Data')))\n",
    "        print(\"    [Sub] 초기 데이터 로드 완료.\")\n",
    "\n",
    "        if \"데이터 준비중입니다.\" in initial_tbody.text or \"데이터가 없습니다.\" in initial_tbody.text:\n",
    "            print(\"    [Sub] '데이터 준비중'이거나 데이터가 없어 이 항목을 스킵합니다.\")\n",
    "            return None\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"    [Sub] 데이터 그리드(sheet1_Data)를 찾는 데 실패했습니다. (미리보기 미제공 추정)\")\n",
    "        return None\n",
    "\n",
    "    # 1. 헤더(컬럼명) 추출\n",
    "    header_elements = driver.find_elements(By.XPATH, \"//thead[@id='sheet1_Header']//th\")\n",
    "    headers = [h.text for h in header_elements if h.text.strip() not in ['', 'No']]\n",
    "\n",
    "    # 2. 모든 데이터 로드를 위한 스크롤 반복\n",
    "    last_row_count = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # [수정] 스크롤 전에 매번 tbody를 새로 찾습니다.\n",
    "            tbody = driver.find_element(By.ID, 'sheet1_Data')\n",
    "            current_row_count = len(tbody.find_elements(By.TAG_NAME, 'tr'))\n",
    "\n",
    "            # [수정] 이전 행 개수와 비교하여 더 이상 로드되지 않으면 탈출\n",
    "            if current_row_count == last_row_count:\n",
    "                print(f\"    [Sub] 스크롤 완료. 총 {current_row_count}개 행 로드됨.\")\n",
    "                break\n",
    "\n",
    "            last_row_count = current_row_count\n",
    "            print(f\"      [Sub] 스크롤... 현재 {current_row_count}개 행 로드됨.\")\n",
    "\n",
    "            # JavaScript를 실행하여 tbody의 스크롤을 맨 아래로 내림\n",
    "            driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", tbody)\n",
    "\n",
    "            # 스크롤 후 새 행이 로드될 때까지 (행 개수가 늘어날 때까지) 대기\n",
    "            scroll_wait.until(\n",
    "                lambda d: len(d.find_element(By.ID, 'sheet1_Data').find_elements(By.TAG_NAME, 'tr')) > current_row_count\n",
    "            )\n",
    "\n",
    "            time.sleep(0.5) # DOM 안정화를 위한 잠깐의 대기\n",
    "\n",
    "        except TimeoutException:\n",
    "            # 5초 동안 새 행이 로드되지 않으면(TimeoutException 발생)\n",
    "            # 모든 데이터를 로드한 것으로 간주하고 루프 종료\n",
    "            print(f\"    [Sub] 스크롤 시간 초과(정상 종료). 총 {last_row_count}개 행 로드됨.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"    [Sub] 스크롤 중 예외 발생: {e}\")\n",
    "            break # 스크롤 중 다른 오류 발생 시 중지\n",
    "\n",
    "    # 3. 스크롤이 완료된 후, 전체 데이터 파싱\n",
    "    print(\"    [Sub] 전체 데이터 파싱 시작...\")\n",
    "    all_data = []\n",
    "\n",
    "    try:\n",
    "        # [수정] 파싱 직전에도 tbody를 \"반드시\" 새로 찾습니다.\n",
    "        final_tbody = driver.find_element(By.ID, 'sheet1_Data')\n",
    "        data_rows = final_tbody.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "        for row in data_rows:\n",
    "            cells = row.find_elements(By.XPATH, './td[position()>1]')\n",
    "            all_data.append([cell.text for cell in cells])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    [Sub] 최종 데이터 파싱 중 오류 발생: {e}\")\n",
    "        return None # 파싱 실패 시 None 반환\n",
    "\n",
    "    # 4. Pandas DataFrame으로 변환\n",
    "    df = pd.DataFrame(all_data, columns=headers)\n",
    "    return df\n",
    "\n",
    "# --- 메인 크롤링 로직 ---\n",
    "\n",
    "# 크롤링할 URL\n",
    "url = \"https://www.bigdata-environment.kr/user/data_market/detail.do?id=1711a0a0-2f03-11ea-bccd-b704c648ae09\"\n",
    "\n",
    "# 제외할 파일 제목\n",
    "EXCLUDED_TITLE = \"행정동별 제과/아이스크림분야 소비인구\"\n",
    "\n",
    "# CSV 파일이 저장될 디렉토리 이름\n",
    "OUTPUT_DIR = \"scraped_data\"\n",
    "\n",
    "# Chrome WebDriver 설정 (자동 설치)\n",
    "# service = Service(ChromeDriverManager().install())\n",
    "# [수정] Chrome WebDriver 설정 부분\n",
    "# 안정적인 실행을 위해 Chrome 옵션을 추가합니다.\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--no-sandbox\") # 샌드박스 모드 비활성화 (권한 충돌 방지)\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\") # 리소스 부족 문제 방지 (주로 Linux/Docker에서 필요하나 Windows에서도 도움됨)\n",
    "chrome_options.add_argument(\"--disable-gpu\") # GPU 가속 비활성화 (GPU 드라이버 충돌 방지)\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\") # 버전은 적당히 최신으로\n",
    "# chrome_options.add_argument(\"--headless\") # (선택사항) 아예 브라우저 창을 띄우지 않고 백그라운드에서 실행\n",
    "\n",
    "print(\"Chrome 드라이버를 옵션과 함께 실행합니다...\")\n",
    "\n",
    "# [수정] 드라이버 설정 시 options=chrome_options 를 전달합니다.\n",
    "# (이전 코드: driver = webdriver.Chrome())\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n",
    "driver = uc.Chrome(options=chrome_options)\n",
    "\n",
    "# 대기 시간 설정\n",
    "wait = WebDriverWait(driver, 10) # 일반 대기 (10초)\n",
    "scroll_wait = WebDriverWait(driver, 5) # 스크롤 후 새 데이터 로드 대기 (5초)\n",
    "\n",
    "# 스크랩한 모든 데이터를 저장할 딕셔너리\n",
    "all_scraped_data = {}\n",
    "\n",
    "print(f\"페이지 로드 중: {url}\")\n",
    "driver.get(url)\n",
    "\n",
    "try:\n",
    "    # '제공데이터' 목록이 있는 <ul> 요소를 찾습니다.\n",
    "    data_list_container = wait.until(\n",
    "        EC.visibility_of_element_located((By.CSS_SELECTOR, \"div.data-provide-list > ul\"))\n",
    "    )\n",
    "\n",
    "    # <li> 태그(각 파일 항목)의 총 개수를 셉니다.\n",
    "    list_items = data_list_container.find_elements(By.TAG_NAME, \"li\")\n",
    "    item_count = len(list_items)\n",
    "    print(f\"'제공데이터' 섹션에서 총 {item_count}개의 항목을 찾았습니다.\")\n",
    "\n",
    "    # 주의: 0부터 시작 (i = 0은 첫 번째 항목)\n",
    "    for i in range(item_count):\n",
    "        print(f\"\\n--- [Main] 항목 {i+1}/{item_count} 처리 중 ---\")\n",
    "\n",
    "        try:\n",
    "            data_list_container = driver.find_element(By.CSS_SELECTOR, \"div.data-provide-list > ul\")\n",
    "            current_item = data_list_container.find_elements(By.TAG_NAME, \"li\")[i]\n",
    "\n",
    "            title_element = current_item.find_element(By.TAG_NAME, \"dt\")\n",
    "            title = title_element.text.strip()\n",
    "            print(f\"  [Main] 제목: {title}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [Main] {i+1}번째 항목을 찾는 데 실패했습니다: {e}. 다음 항목으로 넘어갑니다.\")\n",
    "            continue\n",
    "\n",
    "        # 1. 제외할 제목인지 확인\n",
    "        if title == EXCLUDED_TITLE:\n",
    "            print(f\"  [Main] 제목이 '{EXCLUDED_TITLE}'와 일치하므로 스킵합니다.\")\n",
    "            continue\n",
    "\n",
    "        # 2. 항목 클릭 (제목(dt)을 클릭하여 해당 파일 선택)\n",
    "        try:\n",
    "            print(f\"  [Main] 항목 클릭: '{title}'\")\n",
    "            title_element.click()\n",
    "\n",
    "            wait.until(\n",
    "                EC.element_to_be_clickable((By.ID, \"IBS_data_preview_Button\"))\n",
    "            )\n",
    "            time.sleep(1) # AJAX 로딩을 위한 추가 대기\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  [Main] 항목 클릭 중 오류 발생: {e}. 이 항목을 스킵합니다.\")\n",
    "            continue\n",
    "\n",
    "        # 3. '데이터 미리보기' 탭 클릭\n",
    "        try:\n",
    "            print(\"  [Main] '데이터 미리보기' 탭 클릭...\")\n",
    "            preview_tab = driver.find_element(By.ID, \"IBS_data_preview_Button\")\n",
    "            preview_tab.click()\n",
    "        except Exception as e:\n",
    "            print(f\"  [Main] '데이터 미리보기' 탭 클릭 중 오류 발생: {e}. 이 항목을 스킵합니다.\")\n",
    "            continue\n",
    "\n",
    "        # 4. 스크롤 및 스크래핑 함수 호출\n",
    "        df = scrape_full_preview(driver, wait, scroll_wait)\n",
    "\n",
    "        if df is not None:\n",
    "            print(f\"  [Main] '{title}' 항목의 데이터 {len(df)}행 스크랩 완료.\")\n",
    "            all_scraped_data[title] = df\n",
    "        else:\n",
    "            print(f\"  [Main] '{title}' 항목에서 데이터를 스크랩하지 못했습니다 (미리보기 없음).\")\n",
    "\n",
    "    # --- 모든 루프 종료 ---\n",
    "    print(\"\\n=========================================\")\n",
    "    print(\"모든 크롤링 작업 완료.\")\n",
    "\n",
    "    if not all_scraped_data:\n",
    "        print(\"스크랩된 데이터가 없습니다. (제외 항목만 있었거나 미리보기가 없는 항목뿐이었습니다.)\")\n",
    "    else:\n",
    "        print(f\"총 {len(all_scraped_data)}개의 파일에서 데이터를 스크랩했습니다.\")\n",
    "\n",
    "        # --- CSV 저장 로직 [추가된 부분] ---\n",
    "\n",
    "        # 출력 디렉토리 생성 (없으면 만들기)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        print(f\"\\n'{OUTPUT_DIR}' 폴더에 CSV 파일로 저장합니다...\")\n",
    "\n",
    "        for title, df in all_scraped_data.items():\n",
    "            # 파일명으로 부적절한 문자 제거/변경\n",
    "            safe_title = clean_filename(title)\n",
    "            file_path = os.path.join(OUTPUT_DIR, f\"{safe_title}.csv\")\n",
    "\n",
    "            try:\n",
    "                # CSV 파일로 저장 (UTF-8 with BOM으로 Excel 호환성 확보)\n",
    "                df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "                print(f\"  [저장 완료] '{title}' -> {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  [저장 실패] '{title}' 저장 중 오류 발생: {e}\")\n",
    "\n",
    "        print(\"\\n--- 저장 작업 완료 ---\")\n",
    "        # --- CSV 저장 로직 끝 ---\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"전체 프로세스 중 심각한 오류가 발생했습니다: {e}\")\n",
    "\n",
    "finally:\n",
    "    # 모든 작업이 끝나면 브라우저 종료\n",
    "    print(\"브라우저를 닫습니다.\")\n",
    "    driver.quit()"
   ],
   "id": "10eba3e02df4e50a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# 필수 모듈 임포트\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "def Bigdata_store():\n",
    "    url = \"https://www.bigdata-environment.kr/user/data_market/detail.do?id=1711a0a0-2f03-11ea-bccd-b704c648ae09\"\n",
    "    wd = webdriver.Chrome()\n",
    "    wd.get(url)\n",
    "\n",
    "    wait = WebDriverWait(wd, 10)\n",
    "\n",
    "    try:\n",
    "        # --- 1 ~ 3. (이전과 동일) ---\n",
    "        # (전체보기 클릭 -> 팝업 -> li 클릭 -> 그리드 로드 확인)\n",
    "\n",
    "        # --- (1. '전체보기' 버튼 클릭) ---\n",
    "        xpath_selector = \"//a[@onclick=\\\"modalPop('o','#pop_provide_data')\\\"]\"\n",
    "        view_all_button = wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, xpath_selector))\n",
    "        )\n",
    "        print(\"XPath (onclick)로 '전체보기' 버튼을 찾았습니다. 클릭합니다.\")\n",
    "        wd.execute_script(\"arguments[0].click();\", view_all_button)\n",
    "        print(\"클릭 완료. 팝업 목록을 기다립니다...\")\n",
    "\n",
    "        # --- (2. 팝업 목록에서 li 클릭) ---\n",
    "        # (첫 번째 li를 크롤링한다고 가정 : li:first-child)\n",
    "        li_selector = \"div#pop_provide_data div.provide-data-list ul li:first-child\"\n",
    "        li_element = wait.until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, li_selector))\n",
    "        )\n",
    "        li_text = li_element.text.replace('\\n', ' ')[:40]\n",
    "        print(f\"팝업의 항목 '{li_text}...'을(를) 클릭합니다.\")\n",
    "        li_element.click()\n",
    "        print(\"li 클릭 완료. 데이터 미리보기 그리드를 기다립니다...\")\n",
    "\n",
    "        # --- (3. rMateH5 그리드 로드 대기) ---\n",
    "        grid_cell_selector = \"span.rMateH5__DataGridItemRenderer[id^='DataGridItemRenderer']\"\n",
    "        wait.until(\n",
    "            EC.visibility_of_element_located((By.CSS_SELECTOR, grid_cell_selector))\n",
    "        )\n",
    "        print(\"데이터 그리드 로드를 확인했습니다.\")\n",
    "\n",
    "\n",
    "        # --- 4. 가상 스크롤 처리 (인내심 로직 추가) ---\n",
    "\n",
    "        scroll_container_selector = \"div.rMateH5__VBrowserScrollBar\"\n",
    "        print(f\"'{scroll_container_selector}' 컨테이너를 기준으로 스크롤을 시작합니다.\")\n",
    "\n",
    "        last_scroll_top = -1\n",
    "        all_rows_list = []\n",
    "        seen_rows_set = set()\n",
    "\n",
    "        # [수정] \"인내심\" 카운터 추가\n",
    "        patience_counter = 0\n",
    "\n",
    "        # 헤더/푸터 필터링 목록\n",
    "        HEADER_FOOTER_KEYWORDS = {\n",
    "            \"행정동명\", \"시도명\", \"시군구명\",\"기준일자\",\"성별\",\"성별\",\"연령대\",\"소비인구(명)\",\n",
    "            \"STD_DT\", \"sex_se\", \"year_se\", \"cnsmr_popltn_co\",\n",
    "        }\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # (1. 파싱 로직은 동일)\n",
    "                current_cells = wd.find_elements(By.CSS_SELECTOR, grid_cell_selector)\n",
    "                current_row_data = []\n",
    "\n",
    "                for cell in current_cells:\n",
    "                    try:\n",
    "                        aria_desc = cell.get_attribute(\"aria-describedby\")\n",
    "\n",
    "                        if aria_desc and aria_desc.endswith(\"_Column1\"):\n",
    "                            if current_row_data:\n",
    "                                first_cell_data = str(current_row_data[0]).strip()\n",
    "                                if first_cell_data not in HEADER_FOOTER_KEYWORDS:\n",
    "                                    row_tuple = tuple(current_row_data)\n",
    "                                    if row_tuple not in seen_rows_set:\n",
    "                                        all_rows_list.append(current_row_data)\n",
    "                                        seen_rows_set.add(row_tuple)\n",
    "                                else:\n",
    "                                    print(f\"  (헤더/푸터 행 필터링됨: {first_cell_data}...)\")\n",
    "                            current_row_data = []\n",
    "\n",
    "                        data = cell.get_attribute(\"title\")\n",
    "                        current_row_data.append(data)\n",
    "\n",
    "                    except StaleElementReferenceException:\n",
    "                        continue\n",
    "\n",
    "                if current_row_data:\n",
    "                    first_cell_data = str(current_row_data[0]).strip()\n",
    "                    if first_cell_data not in HEADER_FOOTER_KEYWORDS:\n",
    "                        row_tuple = tuple(current_row_data)\n",
    "                        if row_tuple not in seen_rows_set:\n",
    "                            all_rows_list.append(current_row_data)\n",
    "                            seen_rows_set.add(row_tuple)\n",
    "                    else:\n",
    "                        print(f\"  (헤더/푸터 행 필터링됨: {first_cell_data}...)\")\n",
    "\n",
    "                # (2. 스크롤 로직)\n",
    "                scroll_container = wd.find_element(By.CSS_SELECTOR, scroll_container_selector)\n",
    "                # [수정] 스크롤 속도 조절 (너무 빠르면 로드를 놓칠 수 있음)\n",
    "                wd.execute_script(\"arguments[0].scrollTop += 800\", scroll_container) # 1000 -> 800\n",
    "                time.sleep(0.8) # 1.0 -> 0.8 (조절 가능)\n",
    "\n",
    "                scroll_container = wd.find_element(By.CSS_SELECTOR, scroll_container_selector)\n",
    "                current_scroll_top = wd.execute_script(\"return arguments[0].scrollTop\", scroll_container)\n",
    "\n",
    "                # (3. [핵심 수정] \"인내심\" 체크 로직)\n",
    "                if current_scroll_top == last_scroll_top:\n",
    "                    # 스크롤이 멈췄을 때\n",
    "                    patience_counter += 1\n",
    "                    print(f\"  (스크롤 바닥 도달. 새 데이터 로드 대기... {patience_counter}/3)\")\n",
    "\n",
    "                    # 넉넉하게 3초 대기 (새 네트워크 요청 발생 대기)\n",
    "                    time.sleep(3.0)\n",
    "\n",
    "                    if patience_counter >= 10:\n",
    "                        # 3번(총 9초)을 기다려도 스크롤 위치가 그대로라면,\n",
    "                        # \"진짜\" 데이터의 끝으로 간주하고 종료\n",
    "                        print(\"스크롤 완료. 모든 데이터가 로드되었습니다.\")\n",
    "                        break\n",
    "                else:\n",
    "                    # 스크롤이 성공했다면 (새 데이터가 로드되었거나, 아직 바닥이 아님)\n",
    "                    # \"인내심\" 카운터 초기화\n",
    "                    patience_counter = 0\n",
    "\n",
    "                # (루프가 계속 돌 경우) 현재 위치 저장\n",
    "                last_scroll_top = current_scroll_top\n",
    "                print(f\"새 데이터 로드 중... (현재 스크롤 위치: {current_scroll_top}) (수집된 행: {len(all_rows_list)})\")\n",
    "\n",
    "            except (StaleElementReferenceException, NoSuchElementException) as e:\n",
    "                print(\"  (스크롤바 갱신 감지. 요소를 다시 찾습니다...)\")\n",
    "                time.sleep(0.5)\n",
    "                continue\n",
    "\n",
    "        # --- 5 & 6. (결과 출력 및 CSV 저장 로직 - 이전과 동일) ---\n",
    "        print(\"\\n중복 제거 및 헤더/푸터 필터링 완료.\")\n",
    "\n",
    "        print(\"\\n--- 데이터 크롤링 완료 ---\")\n",
    "\n",
    "        if all_rows_list:\n",
    "            print(f\"총 {len(all_rows_list)}개의 고유한 *데이터 행*을 (순서대로) 추출했습니다.\")\n",
    "\n",
    "            df = pd.DataFrame(all_rows_list)\n",
    "            try:\n",
    "                # (첫 번째 li 데이터라고 가정)\n",
    "                file_name = \"icecream_sales_data_3.csv\"\n",
    "\n",
    "                df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
    "\n",
    "                print(f\"\\n[성공] 데이터가 {file_name} 파일로 저장되었습니다.\")\n",
    "                print(f\" (Python 스크립트와 같은 폴더에 저장됨)\")\n",
    "\n",
    "            except PermissionError:\n",
    "                print(f\"\\n[오류] CSV 파일 저장 실패: {file_name}이(가) 다른 프로그램에서 열려 있습니다.\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n[오류] CSV 파일 저장 중 오류 발생: {e}\")\n",
    "\n",
    "            print(df.head())\n",
    "            print(f\"\\n...\")\n",
    "            print(df.tail())\n",
    "        else:\n",
    "            print(\"추출된 데이터가 없습니다.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"전체 프로세스 중 오류 발생: {e}\")\n",
    "\n",
    "    finally:\n",
    "        print(\"\\n작업 완료. 브라우저를 닫습니다.\")\n",
    "        wd.quit()\n",
    "\n",
    "# 함수 실행\n",
    "if __name__ == \"__main__\":\n",
    "    Bigdata_store()"
   ],
   "id": "12b8549e6dccdfb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- 1. [필수] 사용자 입력: \"Headers\" 탭에서 복사 ---\n",
    "\n",
    "# 1-1. 'Headers' 탭의 'General' 섹션에서 'Request URL'을 복사\n",
    "API_URL = \"https://www.bigdata-environment.kr/user/data_market/process.file.do\" # (아마 이 주소일겁니다)\n",
    "\n",
    "# 1-2. 'Headers' 탭의 'Request Headers' 섹션에서 값 복사\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36', # (아까 찾은 User-Agent)\n",
    "    'Referer': 'https://www.bigdata-environment.kr/user/data_market/detail.do?id=1711a0a0-2f03-11ea-bccd-b704c648ae09', # (아까 찾은 Referer)\n",
    "    'Cookie': 'WMONID=jB_uE_UQAJr; _fwb=216EJGRW08QlWgeh7aFjQAY.1761101174207; _gid=GA1.2.716307017.1761101175; chatbot_cookie=done; JSESSIONID=706CFBEEC46BAFEDE61A38B55C842EEE; wcs_bt=1e8330f8ae88d3:1761289464; _gat_gtag_UA_177242718_1=1; _ga_CKPQWV25B6=GS2.1.s1761288862$o12$g1$t1761289464$j59$l0$h0; _ga=GA1.1.1933955848.1761101175',\n",
    "    'Origin': 'https://www.bigdata-environment.kr',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'\n",
    "}\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# ★★★ [최종 완료] ★★★\n",
    "# 다운로드할 3개 데이터셋 목록\n",
    "DATASET_TO_DOWNLOAD = [\n",
    "    {\n",
    "        # [2번 데이터 (원래 3번째 항목)]\n",
    "        \"id\": \"20b9e320-308c-11eb-bc79-3b11eb915d6d\",\n",
    "        \"filename\": \"data_2_2020_10_21_large.csv\",\n",
    "        \"total_pages\": 193 # (1,928,948건)\n",
    "    }\n",
    "]\n",
    "\n",
    "ROWS_PER_PAGE = 10000 # (Payload의 maxRows 값)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "# 세션(Session)을 생성하여 연결 유지\n",
    "session = requests.Session()\n",
    "session.headers.update(REQUEST_HEADERS)\n",
    "\n",
    "print(f\"--- 총 {len(DATASET_TO_DOWNLOAD)}개의 데이터셋 다운로드를 시작합니다. ---\")\n",
    "\n",
    "# 3개의 데이터셋을 순서대로 처리\n",
    "# (import os가 맨 위에 있는지 확인하세요)\n",
    "import os\n",
    "\n",
    "# 3개의 데이터셋을 순서대로 처리\n",
    "for dataset in DATASET_TO_DOWNLOAD:\n",
    "\n",
    "    current_id = dataset[\"id\"]\n",
    "    current_filename = dataset[\"filename\"]\n",
    "    total_pages = dataset[\"total_pages\"]\n",
    "\n",
    "    print(f\"\\n\\n--- 작업 시작: {current_filename} (ID: {current_id}) ---\")\n",
    "    print(f\"총 {total_pages} 페이지, {ROWS_PER_PAGE * (total_pages - 1)}+ 건의 데이터를 수집합니다.\")\n",
    "    print(f\"데이터는 수집 즉시 '{current_filename}' 파일에 누적 저장됩니다.\")\n",
    "\n",
    "    # [수정] 전체 리스트 대신 총 수집 건수 카운터로 변경\n",
    "    total_records_collected = 0\n",
    "\n",
    "    # [수정] 이어받기 기능: 만약 파일이 이미 존재하면, 어디까지 받았는지 확인\n",
    "    # (간단한 구현: 우선 기존 파일을 삭제하고 새로 시작합니다.)\n",
    "    # (더 복잡한 이어받기를 원하시면 말씀해주세요)\n",
    "    if os.path.exists(current_filename):\n",
    "        print(f\"[경고] '{current_filename}' 파일이 이미 존재합니다. 이어받기 대신, 기존 파일을 삭제하고 새로 시작합니다.\")\n",
    "        os.remove(current_filename)\n",
    "\n",
    "    if total_pages <= 0:\n",
    "        print(\"[오류] total_pages가 0보다 커야 합니다. 이 데이터셋을 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # 1페이지부터 마지막 페이지까지 순차적으로 요청\n",
    "        for page in range(1, total_pages + 1):\n",
    "            payload = {\n",
    "                'TP': 'one_sample',\n",
    "                'id': current_id,\n",
    "                'maxRows': ROWS_PER_PAGE,\n",
    "                'currentPage': page\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = session.post(API_URL, data=payload)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    rows = data.get('preview_data') # <-- 키 이름 확인 필요\n",
    "\n",
    "                    if rows and len(rows) > 0:\n",
    "                        # [핵심 수정] 데이터를 리스트에 담지 않고 DataFrame으로 바로 변환\n",
    "                        df_chunk = pd.DataFrame(rows)\n",
    "\n",
    "                        # [핵심 수정] 1페이지(파일이 없을 때)는 헤더와 함께 'w'(쓰기) 모드로,\n",
    "                        # 2페이지부터는 헤더 없이 'a'(추가) 모드로 저장\n",
    "\n",
    "                        # os.path.exists()로 확인하는 것이 'page == 1'보다\n",
    "                        # 중단 후 재시작 시 더 안전합니다.\n",
    "                        if not os.path.exists(current_filename):\n",
    "                            print(f\"  [저장] Page {page}: '{current_filename}'에 새 파일 생성 (헤더 포함).\")\n",
    "                            df_chunk.to_csv(current_filename, index=False, encoding='utf-8-sig', mode='w')\n",
    "                        else:\n",
    "                            print(f\"  [저장] Page {page}: 기존 파일에 {len(rows)}건 추가.\")\n",
    "                            df_chunk.to_csv(current_filename, index=False, encoding='utf-8-sig', mode='a', header=False)\n",
    "\n",
    "                        total_records_collected += len(rows)\n",
    "                        print(f\"[성공] Page {page}/{total_pages} (이번에 {len(rows)}건, 총 {total_records_collected}건 수집)\")\n",
    "\n",
    "                    else:\n",
    "                        print(f\"[경고] Page {page}/{total_pages} - 응답 성공했으나 데이터가 없음 (rows: {rows})\")\n",
    "                        # (데이터가 없는 페이지가 있다면 루프를 중단할 수도 있음)\n",
    "                        # break\n",
    "                else:\n",
    "                    print(f\"[실패] Page {page}/{total_pages} - 상태 코드: {response.status_code}\")\n",
    "                    print(f\"  (응답 내용: {response.text[:100]}...)\")\n",
    "                    print(\"  [알림] 이 데이터셋의 수집을 중단하고 다음 데이터셋으로 넘어갑니다.\")\n",
    "                    break # 현재 데이터셋 작업 중단\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[오류] Page {page}/{total_pages} 요청 또는 저장 중 오류: {e}\")\n",
    "                print(\"  [알림] 이 데이터셋의 수집을 중단하고 다음 데이터셋으로 넘어갑니다.\")\n",
    "                break # 현재 데이터셋 작업 중단\n",
    "\n",
    "            # (권장) 서버에 부담을 주지 않도록 약간의 대기 시간\n",
    "            time.sleep(1.0) # 0.5초 -> 1.0초 (안정성을 위해 1초로 늘림)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n사용자에 의해 중단. 현재까지 저장된 파일은 남아있습니다.\")\n",
    "        # 루프가 강제 중단되어도, 이미 저장된 CSV는 남아있음\n",
    "\n",
    "    # --- 한 데이터셋 작업 완료 시 CSV 저장 (로직 변경) ---\n",
    "    # [수정] 이미 저장이 완료되었으므로, 최종 요약만 출력\n",
    "    if total_records_collected > 0:\n",
    "        print(f\"\\n--- {current_filename} / 총 {total_records_collected}건 수집 완료 ---\")\n",
    "        print(f\"[최종 성공] '{current_filename}' 파일 저장 완료!\")\n",
    "\n",
    "        # (파일이 너무 클 수 있으니 head() 대신 파일 존재 여부만 알림)\n",
    "        # (미리보기를 원하면 아래 3줄의 주석을 해제하세요)\n",
    "        # try:\n",
    "        #     df_preview = pd.read_csv(current_filename, nrows=5)\n",
    "        #     print(f\"\\n[파일 미리보기 (상위 5건)]\\n{df_preview}\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"[알림] 파일 미리보기 중 오류: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"\\n[최종 실패] {current_filename} / 수집된 데이터가 없습니다.\")\n",
    "\n",
    "print(\"\\n\\n--- 모든 작업 완료 ---\")"
   ],
   "id": "8f92c7409a7d9573"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
