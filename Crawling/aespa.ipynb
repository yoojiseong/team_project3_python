{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import time\n",
    "import obspy\n",
    "import numpy as np\n",
    "\n",
    "# 0. 버전 확인 (1.4.2가 나와야 함)\n",
    "print(f\"--- 현재 obspy 버전: {obspy.__version__} ---\")\n",
    "\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "# --- 1. 2024년도 지진 '검색' ---\n",
    "# (ID를 모르므로, 2024년 1월 1일 일본 노토 반도 지진(M7.5)을 검색)\n",
    "print(\"--- 1. 2024년 1월 1일 (M3.0+) 지진 검색 중... ---\")\n",
    "search_start = UTCDateTime(\"2024-01-01T00:00:00\")\n",
    "search_end = UTCDateTime(\"2024-01-02T00:00:00\") # 1월 1일 하루 동안\n",
    "\n",
    "origin_time = None\n",
    "event_lat = None\n",
    "event_lon = None\n",
    "event_id_str = \"N/A\"\n",
    "\n",
    "try:\n",
    "    cat = client.get_events(\n",
    "        starttime=search_start,\n",
    "        endtime=search_end,\n",
    "        minmagnitude=3.0 # M7.0 이상만 검색\n",
    "    )\n",
    "\n",
    "    if not cat:\n",
    "        print(\"  > 2024년 1월 1일에 M3.0+ 지진을 찾을 수 없습니다.\")\n",
    "        print(\"  > 다른 날짜로 시도해보세요.\")\n",
    "        exit()\n",
    "\n",
    "    # 검색된 첫 번째 이벤트(가장 큰 이벤트)를 사용\n",
    "    event = cat[0]\n",
    "\n",
    "    # 파형 수집에 필요한 정보 추출\n",
    "    origin_time = event.origins[0].time\n",
    "    event_lat = event.origins[0].latitude\n",
    "    event_lon = event.origins[0].longitude\n",
    "    event_mag = event.magnitudes[0].mag\n",
    "    # event.resource_id는 객체일 수 있으므로 .id 속성 확인\n",
    "    event_id_str = event.resource_id.id if event.resource_id else \"ID_NOT_FOUND\"\n",
    "\n",
    "    print(f\"  > 검색된 이벤트: {event.event_descriptions[0].text}\")\n",
    "    print(f\"  > Event ID: {event_id_str}\")\n",
    "    print(f\"  > 발생 시각 (UTC): {origin_time}\")\n",
    "    print(f\"  > 규모: {event_mag}\")\n",
    "    print(f\"  > 위치: {event_lat}, {event_lon}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  > get_events() 오류 (이벤트 검색 실패): {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. '파형(Waveform)' 데이터 수집 ---\n",
    "# (1단계에서 찾은 시간, 위치를 기준으로 파형을 요청)\n",
    "\n",
    "print(f\"\\n--- 2. '{event_id_str}'의 파형(Waveform) 데이터 수집 시도... ---\")\n",
    "print(\"(조건: 발생 후 10분간, 진앙 5도 이내, BH? 채널)\")\n",
    "\n",
    "st = None # 파형 묶음 (Stream)\n",
    "\n",
    "try:\n",
    "    st = client.get_waveforms(\n",
    "        network=\"*\",        # 모든 네트워크\n",
    "        station=\"*\",      # 모든 관측소\n",
    "        location=\"*\",     # 모든 위치 코드\n",
    "        channel=\"BH?\",      # 광대역 채널 (BHZ, BHN, BHE)\n",
    "        starttime=origin_time,              # 1. 검색된 지진 발생 시각\n",
    "        endtime=origin_time + 600,          # 10분(600초) 동안\n",
    "        latitude=event_lat,                 # 2. 검색된 위도\n",
    "        longitude=event_lon,                # 3. 검색된 경도\n",
    "        maxradius=5.0                       # 반경 5도 이내\n",
    "    )\n",
    "\n",
    "    if st:\n",
    "        print(f\"\\n--- 3. 파형 수집 성공! ---\")\n",
    "        print(f\"  > 총 {len(st)} 개의 파형(채널)을 다운로드했습니다.\")\n",
    "\n",
    "        # --- 4. \"수치화된 데이터\" (Number Array) 확인 ---\n",
    "        print(\"\\n--- 4. '수치화된 데이터' (숫자 배열) 확인 ---\")\n",
    "\n",
    "        tr = st[0] # 첫 번째 파형을 샘플로 확인\n",
    "        print(f\"  > 첫 번째 파형 ID: {tr.id}\")\n",
    "\n",
    "        # <<< 이것이 바로 모델에 넣을 \"숫자 배열\"입니다 >>>\n",
    "        number_array = tr.data\n",
    "\n",
    "        print(f\"  > 이 파형의 총 숫자 개수: {len(number_array)} 개\")\n",
    "        print(f\"  > 실제 숫자 배열 (앞 10개): {number_array[:10]}\")\n",
    "\n",
    "        # --- 5. 특징(Feature) 계산 예시 ---\n",
    "        print(\"\\n--- 5. 이 '숫자 배열'로 특징(Feature) 계산하기 (예시) ---\")\n",
    "\n",
    "        max_amplitude = np.max(np.abs(number_array))\n",
    "\n",
    "        print(f\"  > (예시 특징) 최대 진폭 (Max Amplitude): {max_amplitude:.2f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"\\n--- 3. 수집 실패 (데이터 없음) ---\")\n",
    "        print(\"  > 이 조건(2024년 1월 1일)에 맞는 파형 데이터가 서버에 없습니다.\")\n",
    "\n",
    "except AttributeError as ae:\n",
    "    print(f\"\\n--- !!! 심각한 오류 발생 (AttributeError) !!! ---\")\n",
    "    print(f\"  > 오류: {ae}\")\n",
    "    print(f\"  > 원인: 'obspy.Client'가 'get_waveforms' 함수도 찾지 못합니다.\")\n",
    "    print(f\"  > 해결: Python 3.11 가상환경 폴더(venv)를 삭제하고 2단계부터 다시 시도해야 합니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- !!! 오류 발생 !!! ---\")\n",
    "    print(f\"  > 오류: {e}\")\n",
    "    if \"No data available\" in str(e) or \"204\" in str(e):\n",
    "        print(\"  > (원인: 이 조건에 맞는 데이터가 서버에 없습니다.)\")\n",
    "    else:\n",
    "        print(\"  > (원인: 그 외 오류. 네트워크 문제일 수 있습니다.)\")"
   ],
   "id": "c8b6fb9da9f519de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import time\n",
    "import obspy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(f\"--- 현재 obspy 버전: {obspy.__version__} ---\")\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "# --- 1. 2024년도 지진 '검색' ---\n",
    "print(\"--- 1. 2024년 1월 1일 (M3.0+) 지진 검색 중... ---\")\n",
    "search_start = UTCDateTime(\"2024-02-01T00:00:00\")\n",
    "search_end = UTCDateTime(\"2024-03-01T00:00:00\")\n",
    "min_mag = 3.0\n",
    "\n",
    "results_list = []\n",
    "\n",
    "try:\n",
    "    cat = client.get_events(\n",
    "        starttime=search_start,\n",
    "        endtime=search_end,\n",
    "        minmagnitude=min_mag\n",
    "    )\n",
    "    if not cat:\n",
    "        print(f\"  > {search_start.date}에 M{min_mag}+ 지진을 찾을 수 없습니다.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"  > 총 {len(cat)} 개의 M{min_mag}+ 지진을 찾았습니다. 파형 수집을 시작합니다.\")\n",
    "\n",
    "    # --- 2. '모든' 지진을 순회 ---\n",
    "    for i, event in enumerate(cat):\n",
    "\n",
    "        event_time = None\n",
    "        event_lat = None\n",
    "        event_lon = None\n",
    "        event_mag = None\n",
    "        event_depth_km = None\n",
    "        event_id_str = \"N/A\"\n",
    "\n",
    "        # 검색할 반경 (단위: 도)\n",
    "        search_radius_deg = 2.0\n",
    "\n",
    "        try:\n",
    "            # --- 3. 기본 지진 정보 추출 ---\n",
    "            origin = event.origins[0]\n",
    "            event_time = origin.time\n",
    "            event_lat = origin.latitude\n",
    "            event_lon = origin.longitude\n",
    "            event_depth_km = origin.depth / 1000.0 if origin.depth else 0.0\n",
    "            event_mag = event.magnitudes[0].mag if event.magnitudes else 0.0\n",
    "\n",
    "            if event.resource_id:\n",
    "                event_id_str = str(event.resource_id.id).split('=')[-1]\n",
    "            else:\n",
    "                event_id_str = f\"event_{i}\"\n",
    "\n",
    "            print(f\"\\n--- [{i+1}/{len(cat)}] 이벤트 처리 중 (ID: {event_id_str}, Mag: {event_mag}) ---\")\n",
    "\n",
    "            # --- 4. (!!! 여기가 수정된 부분 1 !!!) ---\n",
    "            # 먼저 지진 근처의 '관측소(Station)' 목록을 가져옵니다.\n",
    "            print(f\"  > 4a. 반경 {search_radius_deg}도 이내의 관측소 검색 중...\")\n",
    "\n",
    "            stations_inventory = client.get_stations(\n",
    "                starttime=event_time,\n",
    "                endtime=event_time + 300, # 5분\n",
    "                latitude=event_lat,       # get_stations는 이 파라미터가 작동해야 합니다.\n",
    "                longitude=event_lon,\n",
    "                maxradius=search_radius_deg,\n",
    "                level=\"station\"\n",
    "            )\n",
    "\n",
    "            if not stations_inventory or len(stations_inventory) == 0:\n",
    "                print(\"  > 근처에 관측소를 찾지 못했습니다. (데이터 없음). 다음 이벤트로 넘어갑니다.\")\n",
    "                continue\n",
    "\n",
    "            # 관측소 목록에서 '네트워크'와 '관측소' 코드만 추출합니다.\n",
    "            network_codes = set()\n",
    "            station_codes = set()\n",
    "            for network in stations_inventory:\n",
    "                network_codes.add(network.code)\n",
    "                for station in network:\n",
    "                    station_codes.add(station.code)\n",
    "\n",
    "            if not station_codes:\n",
    "                 print(\"  > 관측소 코드를 추출하지 못했습니다. 다음 이벤트로 넘어갑니다.\")\n",
    "                 continue\n",
    "\n",
    "            # 콤마(,)로 구분된 문자열로 변환 (e.g., \"JP,KR\", \"HON,INU\")\n",
    "            net_str = \",\".join(network_codes)\n",
    "            sta_str = \",\".join(station_codes)\n",
    "\n",
    "            print(f\"  > 4b. 찾은 관측소({len(station_codes)}개)의 파형 요청 중...\")\n",
    "\n",
    "            # --- 5. (!!! 여기가 수정된 부분 2 !!!) ---\n",
    "            # 'minlatitude' 대신 'network', 'station' 코드로 파형 요청\n",
    "            st = client.get_waveforms(\n",
    "                network=net_str,    # (수정) 네트워크 코드로 필터\n",
    "                station=sta_str,    # (수정) 관측소 코드로 필터\n",
    "                location=\"*\",\n",
    "                channel=\"BHZ\",\n",
    "                starttime=event_time,\n",
    "                endtime=event_time + 300\n",
    "                # (삭제) minlatitude, maxlatitude, minlongitude, maxlongitude\n",
    "            )\n",
    "\n",
    "            max_amplitude = 0.0\n",
    "\n",
    "            if st:\n",
    "                max_amps_per_station = [np.max(np.abs(tr.data)) for tr in st if len(tr.data) > 0]\n",
    "                if max_amps_per_station:\n",
    "                    max_amplitude = np.max(max_amps_per_station)\n",
    "                print(f\"  > 파형 수집 성공. 최대 진폭 (Max Amp): {max_amplitude:.2f}\")\n",
    "            else:\n",
    "                print(f\"  > 이 이벤트의 파형 데이터를 찾을 수 없습니다 (No data available).\")\n",
    "\n",
    "            # --- 6. CSV 저장을 위해 리스트에 '결과' 추가 ---\n",
    "            results_list.append({\n",
    "                \"event_id\": event_id_str,\n",
    "                \"time_utc\": event_time.isoformat(),\n",
    "                \"latitude\": event_lat,\n",
    "                \"longitude\": event_lon,\n",
    "                \"magnitude\": event_mag,\n",
    "                \"depth_km\": event_depth_km,\n",
    "                \"max_amplitude_bhz\": max_amplitude\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # 만약 get_stations()에서도 'latitude' 오류가 난다면, obspy.Client 자체가 완전히 고장 난 것입니다.\n",
    "            print(f\"  > 이벤트 처리 중 치명적 오류 발생: {e}\")\n",
    "            continue\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  > get_events() 오류 (이벤트 검색 실패): {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 7. 모든 작업 완료 후, 리스트를 Pandas DataFrame으로 변환 ---\n",
    "print(\"\\n--- 7. 모든 이벤트 처리 완료. CSV 파일로 저장합니다. ---\")\n",
    "\n",
    "if results_list:\n",
    "    df = pd.DataFrame(results_list)\n",
    "    output_filename = f\"earthquake_features_{search_start.date}.csv\"\n",
    "    output_path = os.path.join(os.getcwd(), output_filename)\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"  > 성공! 총 {len(df)} 개의 이벤트 데이터를\")\n",
    "    print(f\"  > '{output_path}' 파일로 저장했습니다.\")\n",
    "    print(\"\\n[CSV 샘플 데이터]\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"  > 저장할 데이터가 없습니다. (모든 이벤트에서 파형 수집 실패)\")\n",
    "\n",
    "print(\"\\n--- 작업 종료 ---\")"
   ],
   "id": "f7065cb8452c80e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import time\n",
    "import obspy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(f\"--- 현재 obspy 버전: {obspy.__version__} ---\")\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "# --- 1. 2024년도 지진 '검색' ---\n",
    "print(\"--- 1. 2024년 1월 1일 (M3.0+) 지진 검색 중... ---\")\n",
    "search_start = UTCDateTime(\"2024-01-01T00:00:00\")\n",
    "search_end = UTCDateTime(\"2024-01-02T00:00:00\")\n",
    "min_mag = 3.0\n",
    "\n",
    "results_list = []\n",
    "\n",
    "try:\n",
    "    # (수정) 쓰나미 정보를 포함할 수 있도록 includeall=True 추가\n",
    "    cat = client.get_events(\n",
    "        starttime=search_start,\n",
    "        endtime=search_end,\n",
    "        minmagnitude=min_mag,\n",
    "        includeallorigins=True # 혹시 모를 상세 정보를 위해\n",
    "    )\n",
    "    if not cat:\n",
    "        print(f\"  > {search_start.date}에 M{min_mag}+ 지진을 찾을 수 없습니다.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"  > 총 {len(cat)} 개의 M{min_mag}+ 지진을 찾았습니다. 파형 수집을 시작합니다.\")\n",
    "\n",
    "    # --- 2. '모든' 지진을 순회 ---\n",
    "    for i, event in enumerate(cat):\n",
    "\n",
    "        event_time = None\n",
    "        event_lat = None\n",
    "        event_lon = None\n",
    "        event_mag = None\n",
    "        event_depth_km = None\n",
    "        event_id_str = \"N/A\"\n",
    "\n",
    "        search_radius_deg = 2.0\n",
    "\n",
    "        try:\n",
    "            # --- 3. 기본 지진 정보 추출 ---\n",
    "            origin = event.origins[0]\n",
    "            event_time = origin.time\n",
    "            event_lat = origin.latitude\n",
    "            event_lon = origin.longitude\n",
    "            event_depth_km = origin.depth / 1000.0 if origin.depth else 0.0\n",
    "            event_mag = event.magnitudes[0].mag if event.magnitudes else 0.0\n",
    "\n",
    "            if event.resource_id:\n",
    "                event_id_str = str(event.resource_id.id).split('=')[-1]\n",
    "            else:\n",
    "                event_id_str = f\"event_{i}\"\n",
    "\n",
    "            print(f\"\\n--- [{i+1}/{len(cat)}] 이벤트 처리 중 (ID: {event_id_str}, Mag: {event_mag}) ---\")\n",
    "\n",
    "            # --- (!!! 여기가 새로 추가된 부분 !!!) ---\n",
    "            # --- 4. 쓰나미 정보 확인 ---\n",
    "            tsunami_flag = 0 # 0 = 쓰나미 아님, 1 = 쓰나미 가능성\n",
    "\n",
    "            # 4a. 이벤트 타입 확인\n",
    "            if event.event_type == \"tsunami\":\n",
    "                tsunami_flag = 1\n",
    "\n",
    "            # 4b. 이벤트 설명(description) 텍스트 확인\n",
    "            if tsunami_flag == 0 and event.event_descriptions:\n",
    "                for desc in event.event_descriptions:\n",
    "                    if \"tsunami\" in desc.text.lower():\n",
    "                        tsunami_flag = 1\n",
    "                        break # 'tsunami' 단어를 찾았으면 중단\n",
    "\n",
    "            if tsunami_flag == 1:\n",
    "                print(\"  > !!! 쓰나미(tsunami) 키워드 발견! !!!\")\n",
    "            # --- (추가 끝) ---\n",
    "\n",
    "\n",
    "            # --- 5. 근처 '관측소(Station)' 목록 검색 ---\n",
    "            print(f\"  > 5a. 반경 {search_radius_deg}도 이내의 관측소 검색 중...\")\n",
    "\n",
    "            stations_inventory = client.get_stations(\n",
    "                starttime=event_time,\n",
    "                endtime=event_time + 300,\n",
    "                latitude=event_lat,\n",
    "                longitude=event_lon,\n",
    "                maxradius=search_radius_deg,\n",
    "                level=\"station\"\n",
    "            )\n",
    "\n",
    "            if not stations_inventory or len(stations_inventory) == 0:\n",
    "                print(\"  > 근처에 관측소를 찾지 못했습니다. (데이터 없음).\")\n",
    "                # (수정) 관측소가 없어도 지진 정보(쓰나미 플래그 포함)는 저장\n",
    "                max_amplitude = 0.0 # 파형이 없으므로 0\n",
    "\n",
    "            else:\n",
    "                # --- 6. 파형(Waveform) 데이터 수집 ---\n",
    "                network_codes = {net.code for net in stations_inventory}\n",
    "                station_codes = {sta.code for net in stations_inventory for sta in net}\n",
    "\n",
    "                net_str = \",\".join(network_codes)\n",
    "                sta_str = \",\".join(station_codes)\n",
    "\n",
    "                print(f\"  > 6b. 찾은 관측소({len(station_codes)}개)의 파형 요청 중...\")\n",
    "\n",
    "                st = client.get_waveforms(\n",
    "                    network=net_str,\n",
    "                    station=sta_str,\n",
    "                    location=\"*\",\n",
    "                    channel=\"BHZ\",\n",
    "                    starttime=event_time,\n",
    "                    endtime=event_time + 300\n",
    "                )\n",
    "\n",
    "                max_amplitude = 0.0\n",
    "\n",
    "                if st:\n",
    "                    max_amps_per_station = [np.max(np.abs(tr.data)) for tr in st if len(tr.data) > 0]\n",
    "                    if max_amps_per_station:\n",
    "                        max_amplitude = np.max(max_amps_per_station)\n",
    "                    print(f\"  > 파형 수집 성공. 최대 진폭 (Max Amp): {max_amplitude:.2f}\")\n",
    "                else:\n",
    "                    print(f\"  > 이 이벤트의 파형 데이터를 찾을 수 없습니다 (No data available).\")\n",
    "\n",
    "            # --- 7. CSV 저장을 위해 리스트에 '결과' 추가 ---\n",
    "            results_list.append({\n",
    "                \"event_id\": event_id_str,\n",
    "                \"time_utc\": event_time.isoformat(),\n",
    "                \"latitude\": event_lat,\n",
    "                \"longitude\": event_lon,\n",
    "                \"magnitude\": event_mag,\n",
    "                \"depth_km\": event_depth_km,\n",
    "                \"max_amplitude_bhz\": max_amplitude,\n",
    "                \"tsunami_flag\": tsunami_flag  # (!!! 추가된 컬럼 !!!)\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  > 이벤트 처리 중 치명적 오류 발생: {e}\")\n",
    "            continue\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  > get_events() 오류 (이벤트 검색 실패): {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 8. 모든 작업 완료 후, 리스트를 Pandas DataFrame으로 변환 ---\n",
    "print(\"\\n--- 8. 모든 이벤트 처리 완료. CSV 파일로 저장합니다. ---\")\n",
    "\n",
    "if results_list:\n",
    "    df = pd.DataFrame(results_list)\n",
    "    output_filename = f\"earthquake_features_tsunami_{search_start.date}.csv\" # 파일 이름 변경\n",
    "    output_path = os.path.join(os.getcwd(), output_filename)\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"  > 성공! 총 {len(df)} 개의 이벤트 데이터를\")\n",
    "    print(f\"  > '{output_path}' 파일로 저장했습니다.\")\n",
    "    print(\"\\n[CSV 샘플 데이터]\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"  > 저장할 데이터가 없습니다. (모든 이벤트에서 파형 수집 실패)\")\n",
    "\n",
    "print(\"\\n--- 작업 종료 ---\")"
   ],
   "id": "cdb6aafb2bb624dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "import json\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "# --- 1. CSV의 첫 번째 줄 데이터 (규모 4.3) ---\n",
    "event_lat_from_iris = 37.3362\n",
    "event_lon_from_iris = 136.9617\n",
    "event_time_from_iris = UTCDateTime(\"2024-01-01T22:13:30.239000\")\n",
    "\n",
    "print(f\"--- IRIS에서 찾은 이벤트 정보 (M 4.3) ---\")\n",
    "print(f\"Time: {event_time_from_iris}\")\n",
    "print(f\"Lat: {event_lat_from_iris}, Lon: {event_lon_from_iris}\\n\")\n",
    "\n",
    "# --- 2. 이 이벤트를 위한 '시공간 창' 정의 ---\n",
    "search_start_time = event_time_from_iris - 60  # 1분 전\n",
    "search_end_time = event_time_from_iris + 60    # 1분 후\n",
    "search_radius_km = 100 # 100km 반경 (기관 간 위치 오차 감안)\n",
    "\n",
    "# --- 3. USGS API로 '좁은 검색' 요청 ---\n",
    "url = (\n",
    "    f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson\"\n",
    "    f\"&starttime={search_start_time.isoformat()}\"\n",
    "    f\"&endtime={search_end_time.isoformat()}\"\n",
    "    f\"&latitude={event_lat_from_iris}\"\n",
    "    f\"&longitude={event_lon_from_iris}\"\n",
    "    f\"&maxradiuskm={search_radius_km}\"\n",
    "    # (!!!) 바로 이 부분! 최소 규모 필터를 제거해야 합니다 (!!!)\n",
    "    # f\"&minmagnitude=4.5\"  <- 이 코드가 문제였습니다.\n",
    ")\n",
    "\n",
    "print(f\"--- USGS에 '좁은 검색' 요청 ---\")\n",
    "print(f\"URL: {url}\\n\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    num_found = len(data['features'])\n",
    "\n",
    "    if num_found == 1:\n",
    "        # --- 4. 정확히 1개의 이벤트를 찾았을 때 (성공!) ---\n",
    "        feature = data['features'][0]\n",
    "        props = feature['properties']\n",
    "\n",
    "        usgs_tsunami_flag = props['tsunami']\n",
    "        usgs_event_id = feature['id']\n",
    "        usgs_mag = props['mag']\n",
    "\n",
    "        print(f\"  > 성공! USGS에서 매칭되는 이벤트 1개를 찾았습니다.\")\n",
    "        print(f\"  > USGS ID: {usgs_event_id}\") # 아마 'us7000lsyp'가 나올 것입니다.\n",
    "        print(f\"  > USGS Mag: {usgs_mag}\")\n",
    "        print(f\"  > *** 공식 쓰나미 플래그 (tsunami): {usgs_tsunami_flag} ***\")\n",
    "\n",
    "        if usgs_tsunami_flag == 1:\n",
    "            print(\"  > (결과: 쓰나미 발생 이벤트)\")\n",
    "        else:\n",
    "            print(\"  > (결과: 쓰나미 미발생 이벤트)\")\n",
    "\n",
    "    elif num_found > 1:\n",
    "        print(f\"  > 오류: 좁은 범위에서 {num_found}개의 이벤트가 발견되었습니다.\")\n",
    "        # 이 경우, magnitude가 가장 비슷한 것을 고르는 로직이 추가로 필요합니다.\n",
    "    else:\n",
    "        print(\"  > 오류: USGS 카탈로그에서 매칭되는 이벤트를 찾지 못했습니다.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  > 요청 중 오류 발생: {e}\")"
   ],
   "id": "d58fcdc16225303d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta # obspy 대신 표준 라이브러리 사용\n",
    "\n",
    "# --- 1. 파일 불러오기 ---\n",
    "input_filename = \"earthquake_features_2024-01-01.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(input_filename)\n",
    "    print(f\"--- '{input_filename}' 파일을 성공적으로 불러왔습니다. (총 {len(df)}개 이벤트) ---\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"--- 오류: '{input_filename}' 파일을 찾을 수 없습니다. ---\")\n",
    "    exit() # 스크립트 종료\n",
    "\n",
    "# --- 2. USGS API 요청을 위한 설정 ---\n",
    "usgs_api_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "search_radius_km = 100  # IRIS와 USGS 간의 위치 오차를 고려한 검색 반경\n",
    "time_window_seconds = 60 # IRIS와 USGS 간의 시간 오차를 고려한 검색 시간 (±60초)\n",
    "\n",
    "results_tsunami_flags = [] # USGS 플래그를 저장할 리스트\n",
    "\n",
    "print(f\"--- USGS API 조회를 시작합니다 (총 {len(df)}개 이벤트). 이벤트당 약 1초가 소요됩니다. ---\")\n",
    "\n",
    "# --- 3. DataFrame의 '모든 행'을 순회 ---\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    # CSV에서 현재 이벤트 정보 추출\n",
    "    event_lat = row['latitude']\n",
    "    event_lon = row['longitude']\n",
    "    event_mag = row['magnitude']\n",
    "    event_time_str = row['time_utc'] # 문자열로 시간 가져오기\n",
    "\n",
    "    try:\n",
    "        # CSV의 시간 형식: '2024-01-01T22:27:01.373000'\n",
    "        event_time_dt = datetime.strptime(event_time_str, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        # 가끔 마이크로초(.f)가 없는 시간 형식을 대비\n",
    "        try:\n",
    "            event_time_dt = datetime.strptime(event_time_str, '%Y-%m-%dT%H:%M:%S')\n",
    "        except Exception as e:\n",
    "            print(f\"  > 시간 파싱 오류 (행 {index}): {e}. 이 행은 건너뜁니다.\")\n",
    "            results_tsunami_flags.append(np.nan)\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n--- [ {index + 1} / {len(df)} ] 처리 중 (IRIS Mag: {event_mag}) ---\")\n",
    "    print(f\"  > IRIS 시간: {event_time_dt.isoformat()}\")\n",
    "\n",
    "    # USGS 검색을 위한 '시공간 창' 설정 (timedelta 사용)\n",
    "    search_start_dt = event_time_dt - timedelta(seconds=time_window_seconds)\n",
    "    search_end_dt = event_time_dt + timedelta(seconds=time_window_seconds)\n",
    "\n",
    "    # API가 요구하는 ISO 형식으로 변환 (UTC 표기 'Z' 추가)\n",
    "    search_start_iso = search_start_dt.isoformat() + \"Z\"\n",
    "    search_end_iso = search_end_dt.isoformat() + \"Z\"\n",
    "\n",
    "    # API 요청 파라미터 (!!! minmagnitude 필터 제거 !!!)\n",
    "    params = {\n",
    "        'format': 'geojson',\n",
    "        'starttime': search_start_iso,\n",
    "        'endtime': search_end_iso,\n",
    "        'latitude': event_lat,\n",
    "        'longitude': event_lon,\n",
    "        'maxradiuskm': search_radius_km\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # --- 4. USGS API 호출 ---\n",
    "        response = requests.get(usgs_api_url, params=params, timeout=10)\n",
    "\n",
    "        usgs_tsunami_flag = np.nan # 기본값은 '데이터 없음'\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            features = data.get('features', [])\n",
    "\n",
    "            # --- 5. API 응답 결과 처리 ---\n",
    "            if len(features) == 0:\n",
    "                # Case 1: 좁은 범위에서 매칭되는 이벤트 '없음'\n",
    "                print(\"  > USGS 매칭 실패 (0개). 쓰나미 플래그 '0'으로 설정.\")\n",
    "                usgs_tsunami_flag = 0 # 매칭이 안되면 쓰나미가 아니라고 가정\n",
    "\n",
    "            elif len(features) == 1:\n",
    "                # Case 2: 1개로 '정확히' 매칭됨 (Best case)\n",
    "                props = features[0]['properties']\n",
    "                usgs_tsunami_flag = props['tsunami']\n",
    "                print(f\"  > USGS 매칭 성공 (1개)! (ID: {features[0]['id']}, Mag: {props['mag']})\")\n",
    "                print(f\"  > *** 공식 쓰나미 플래그: {usgs_tsunami_flag} ***\")\n",
    "\n",
    "            else:\n",
    "                # Case 3: 2개 이상 '모호하게' 매칭됨\n",
    "                print(f\"  > USGS 매칭 모호함 ({len(features)}개 발견). 규모가 가장 비슷한 이벤트로 매칭 시도...\")\n",
    "\n",
    "                best_match = None\n",
    "                min_mag_diff = float('inf')\n",
    "\n",
    "                # IRIS의 규모(event_mag)와 가장 차이가 적게 나는 이벤트를 찾는다\n",
    "                for f in features:\n",
    "                    usgs_mag = f['properties']['mag']\n",
    "                    if usgs_mag is None: continue\n",
    "\n",
    "                    mag_diff = abs(usgs_mag - event_mag)\n",
    "\n",
    "                    if mag_diff < min_mag_diff:\n",
    "                        min_mag_diff = mag_diff\n",
    "                        best_match = f\n",
    "\n",
    "                if best_match:\n",
    "                    props = best_match['properties']\n",
    "                    usgs_tsunami_flag = props['tsunami']\n",
    "                    print(f\"  > > 최적 매칭: (ID: {best_match['id']}, Mag: {props['mag']})\")\n",
    "                    print(f\"  > *** 공식 쓰나미 플래그: {usgs_tsunami_flag} ***\")\n",
    "                else:\n",
    "                    print(\"  > > 최적 매칭 실패. 플래그 '0'으로 설정.\")\n",
    "                    usgs_tsunami_flag = 0 # 그래도 못찾으면 0\n",
    "\n",
    "        else:\n",
    "            # API 자체가 실패한 경우 (e.g., 500 서버 오류)\n",
    "            print(f\"  > USGS API 요청 실패 (HTTP Status: {response.status_code}).\")\n",
    "            usgs_tsunami_flag = np.nan # '0'이 아니라 '데이터 없음'으로 표기\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        # 네트워크 연결 오류 등\n",
    "        print(f\"  > API 요청 중 예외 발생: {e}\")\n",
    "        usgs_tsunami_flag = np.nan # '데이터 없음'\n",
    "\n",
    "    # 최종 플래그를 리스트에 추가\n",
    "    results_tsunami_flags.append(usgs_tsunami_flag)\n",
    "\n",
    "    # !!! USGS API 서버 과부하를 막기 위해 1초 대기 (매우 중요) !!!\n",
    "    time.sleep(1)\n",
    "\n",
    "# --- 6. 원본 DataFrame에 새로운 컬럼 추가 ---\n",
    "df['tsunami_flag_usgs'] = results_tsunami_flags\n",
    "\n",
    "# --- 7. 새 파일로 저장 ---\n",
    "output_filename = f\"earthquake_features_with_tsunami_usgs.csv\"\n",
    "output_path = os.path.join(os.getcwd(), output_filename)\n",
    "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n--- 작업 완료 ---\")\n",
    "print(f\"  > '{output_path}' 파일에 USGS 쓰나미 플래그가 추가된 데이터를 저장했습니다.\")\n",
    "print(\"\\n[최종 데이터 샘플 (tsunami_flag_usgs 컬럼 확인)]\")\n",
    "print(df.head())\n",
    "\n",
    "# 쓰나미 플래그가 1인 (즉, 쓰나미가 발생한) 이벤트가 있었는지 확인\n",
    "tsunami_events = df[df['tsunami_flag_usgs'] == 1]\n",
    "if not tsunami_events.empty:\n",
    "    print(\"\\n[!!! 쓰나미(1)로 확인된 이벤트 !!!]\")\n",
    "    print(tsunami_events[['time_utc', 'magnitude', 'tsunami_flag_usgs']])\n",
    "else:\n",
    "    print(\"\\n[확인: 이 데이터셋에서 쓰나미(1)로 확인된 이벤트는 없습니다.]\")"
   ],
   "id": "8bb5757efd7b92bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta # obspy 대신 표준 라이브러리 사용\n",
    "\n",
    "# --- 1. 파일 불러오기 ---\n",
    "input_filename = \"earthquake_features_2024-12                    -01.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(input_filename)\n",
    "    print(f\"--- '{input_filename}' 파일을 성공적으로 불러왔습니다. (총 {len(df)}개 이벤트) ---\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"--- 오류: '{input_filename}' 파일을 찾을 수 없습니다. ---\")\n",
    "    exit() # 스크립트 종료\n",
    "\n",
    "# --- 2. USGS API 요청을 위한 설정 ---\n",
    "usgs_api_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "search_radius_km = 100  # IRIS와 USGS 간의 위치 오차를 고려한 검색 반경\n",
    "time_window_seconds = 60 # IRIS와 USGS 간의 시간 오차를 고려한 검색 시간 (±60초)\n",
    "\n",
    "results_tsunami_flags = [] # USGS 플래그를 저장할 리스트\n",
    "\n",
    "print(f\"--- USGS API 조회를 시작합니다 (총 {len(df)}개 이벤트). 이벤트당 약 1초가 소요됩니다. ---\")\n",
    "\n",
    "# --- 3. DataFrame의 '모든 행'을 순회 ---\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    # CSV에서 현재 이벤트 정보 추출\n",
    "    event_lat = row['latitude']\n",
    "    event_lon = row['longitude']\n",
    "    event_mag = row['magnitude']\n",
    "    event_time_str = row['time_utc'] # 문자열로 시간 가져오기\n",
    "\n",
    "    try:\n",
    "        # CSV의 시간 형식: '2024-01-01T22:27:01.373000'\n",
    "        event_time_dt = datetime.strptime(event_time_str, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        # 가끔 마이크로초(.f)가 없는 시간 형식을 대비\n",
    "        try:\n",
    "            event_time_dt = datetime.strptime(event_time_str, '%Y-%m-%dT%H:%M:%S')\n",
    "        except Exception as e:\n",
    "            print(f\"  > 시간 파싱 오류 (행 {index}): {e}. 이 행은 건너뜁니다.\")\n",
    "            results_tsunami_flags.append(np.nan)\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n--- [ {index + 1} / {len(df)} ] 처리 중 (IRIS Mag: {event_mag}) ---\")\n",
    "    print(f\"  > IRIS 시간: {event_time_dt.isoformat()}\")\n",
    "\n",
    "    # USGS 검색을 위한 '시공간 창' 설정 (timedelta 사용)\n",
    "    search_start_dt = event_time_dt - timedelta(seconds=time_window_seconds)\n",
    "    search_end_dt = event_time_dt + timedelta(seconds=time_window_seconds)\n",
    "\n",
    "    # API가 요구하는 ISO 형식으로 변환 (UTC 표기 'Z' 추가)\n",
    "    search_start_iso = search_start_dt.isoformat() + \"Z\"\n",
    "    search_end_iso = search_end_dt.isoformat() + \"Z\"\n",
    "\n",
    "    # API 요청 파라미터 (!!! minmagnitude 필터 제거 !!!)\n",
    "    params = {\n",
    "        'format': 'geojson',\n",
    "        'starttime': search_start_iso,\n",
    "        'endtime': search_end_iso,\n",
    "        'latitude': event_lat,\n",
    "        'longitude': event_lon,\n",
    "        'maxradiuskm': search_radius_km\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # --- 4. USGS API 호출 ---\n",
    "        response = requests.get(usgs_api_url, params=params, timeout=10)\n",
    "\n",
    "        usgs_tsunami_flag = np.nan # 기본값은 '데이터 없음'\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            features = data.get('features', [])\n",
    "\n",
    "            # --- 5. API 응답 결과 처리 ---\n",
    "            if len(features) == 0:\n",
    "                # Case 1: 좁은 범위에서 매칭되는 이벤트 '없음'\n",
    "                print(\"  > USGS 매칭 실패 (0개). 쓰나미 플래그 '0'으로 설정.\")\n",
    "                usgs_tsunami_flag = 0 # 매칭이 안되면 쓰나미가 아니라고 가정\n",
    "\n",
    "            elif len(features) == 1:\n",
    "                # Case 2: 1개로 '정확히' 매칭됨 (Best case)\n",
    "                props = features[0]['properties']\n",
    "                usgs_tsunami_flag = props['tsunami']\n",
    "                print(f\"  > USGS 매칭 성공 (1개)! (ID: {features[0]['id']}, Mag: {props['mag']})\")\n",
    "                print(f\"  > *** 공식 쓰나미 플래그: {usgs_tsunami_flag} ***\")\n",
    "\n",
    "            else:\n",
    "                # Case 3: 2개 이상 '모호하게' 매칭됨\n",
    "                print(f\"  > USGS 매칭 모호함 ({len(features)}개 발견). 규모가 가장 비슷한 이벤트로 매칭 시도...\")\n",
    "\n",
    "                best_match = None\n",
    "                min_mag_diff = float('inf')\n",
    "\n",
    "                # IRIS의 규모(event_mag)와 가장 차이가 적게 나는 이벤트를 찾는다\n",
    "                for f in features:\n",
    "                    usgs_mag = f['properties']['mag']\n",
    "                    if usgs_mag is None: continue\n",
    "\n",
    "                    mag_diff = abs(usgs_mag - event_mag)\n",
    "\n",
    "                    if mag_diff < min_mag_diff:\n",
    "                        min_mag_diff = mag_diff\n",
    "                        best_match = f\n",
    "\n",
    "                if best_match:\n",
    "                    props = best_match['properties']\n",
    "                    usgs_tsunami_flag = props['tsunami']\n",
    "                    print(f\"  > > 최적 매칭: (ID: {best_match['id']}, Mag: {props['mag']})\")\n",
    "                    print(f\"  > *** 공식 쓰나미 플래그: {usgs_tsunami_flag} ***\")\n",
    "                else:\n",
    "                    print(\"  > > 최적 매칭 실패. 플래그 '0'으로 설정.\")\n",
    "                    usgs_tsunami_flag = 0 # 그래도 못찾으면 0\n",
    "\n",
    "        else:\n",
    "            # API 자체가 실패한 경우 (e.g., 500 서버 오류)\n",
    "            print(f\"  > USGS API 요청 실패 (HTTP Status: {response.status_code}).\")\n",
    "            usgs_tsunami_flag = np.nan # '0'이 아니라 '데이터 없음'으로 표기\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        # 네트워크 연결 오류 등\n",
    "        print(f\"  > API 요청 중 예외 발생: {e}\")\n",
    "        usgs_tsunami_flag = np.nan # '데이터 없음'\n",
    "\n",
    "    # 최종 플래그를 리스트에 추가\n",
    "    results_tsunami_flags.append(usgs_tsunami_flag)\n",
    "\n",
    "    # !!! USGS API 서버 과부하를 막기 위해 1초 대기 (매우 중요) !!!\n",
    "    time.sleep(1)\n",
    "\n",
    "# --- 6. 원본 DataFrame에 새로운 컬럼 추가 ---\n",
    "df['tsunami_flag_usgs'] = results_tsunami_flags\n",
    "\n",
    "# --- 7. 새 파일로 저장 ---\n",
    "output_filename = f\"earthquake_features_2024-12-01_Tsunami.csv\"\n",
    "output_path = os.path.join(os.getcwd(), output_filename)\n",
    "df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n--- 작업 완료 ---\")\n",
    "print(f\"  > '{output_path}' 파일에 USGS 쓰나미 플래그가 추가된 데이터를 저장했습니다.\")\n",
    "print(\"\\n[최종 데이터 샘플 (tsunami_flag_usgs 컬럼 확인)]\")\n",
    "print(df.head())\n",
    "\n",
    "# 쓰나미 플래그가 1인 (즉, 쓰나미가 발생한) 이벤트가 있었는지 확인\n",
    "tsunami_events = df[df['tsunami_flag_usgs'] == 1]\n",
    "if not tsunami_events.empty:\n",
    "    print(\"\\n[!!! 쓰나미(1)로 확인된 이벤트 !!!]\")\n",
    "    print(tsunami_events[['time_utc', 'magnitude', 'tsunami_flag_usgs']])\n",
    "else:\n",
    "    print(\"\\n[확인: 이 데이터셋에서 쓰나미(1)로 확인된 이벤트는 없습니다.]\")"
   ],
   "id": "629d1bc114ef7b7c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
