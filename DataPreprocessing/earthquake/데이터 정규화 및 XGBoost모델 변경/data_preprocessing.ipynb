{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-02T12:24:21.080669Z",
     "start_time": "2025-11-02T12:24:21.067606Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# 1. 원본 데이터 파일\n",
    "INPUT_CSV = 'final_802_data_for_training.csv'\n",
    "TARGET_COLUMN = 'tsunami' # 타겟(y) 컬럼 이름\n",
    "\n",
    "# 2. ★★★ 예측 스크립트와 호환되는 6개 특성 ★★★\n",
    "FEATURE_COLUMNS = [\n",
    "    'magnitude',\n",
    "    'depth',\n",
    "    'is_ocean',\n",
    "    'is_steep_slope',\n",
    "    'horizontal_count_1y_full', # (h_count)\n",
    "    'vertical_count_1y_full'    # (v_count)\n",
    "]\n",
    "\n",
    "# 3. 저장할 파일 이름\n",
    "SCALER_OUTPUT = 'tsunami_scaler.joblib' # (스케일러 모델 파일)\n",
    "SCALED_DATA_OUTPUT = 'scaled_model_data_6features.csv' # (정형화된 데이터 파일)\n",
    "\n",
    "def create_scaler_and_scaled_data():\n",
    "    \"\"\"\n",
    "    (수정됨) 6-feature 스케일러를 생성하고, 스케일러(.joblib)와\n",
    "    정형화된 데이터(.csv)를 모두 저장합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 원본 데이터 로드\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "        print(f\"'{INPUT_CSV}' 로드 완료.\")\n",
    "\n",
    "        # 6개 특성(X)과 타겟(y) 분리\n",
    "        X_features = df[FEATURE_COLUMNS]\n",
    "        y_target = df[TARGET_COLUMN]\n",
    "\n",
    "        print(f\"예측 호환용 6개 특성 선택 완료: {FEATURE_COLUMNS}\")\n",
    "\n",
    "        # StandardScaler 생성 및 학습(fit)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_features)\n",
    "\n",
    "        print(\"StandardScaler 학습(fit) 완료.\")\n",
    "\n",
    "        # --- 작업 1: 스케일러(모델) 파일 저장 ---\n",
    "        joblib.dump(scaler, SCALER_OUTPUT)\n",
    "        print(f\"\\n[작업 1] 성공! 스케일러가 '{SCALER_OUTPUT}' 파일로 저장되었습니다.\")\n",
    "\n",
    "\n",
    "        # --- 작업 2: 정형화된 데이터 파일 저장 (사용자 요청 추가) ---\n",
    "        print(f\"\\n[작업 2] 데이터를 변환하여 '{SCALED_DATA_OUTPUT}' 파일로 저장합니다...\")\n",
    "\n",
    "        # 1. 스케일러로 6개 특성 데이터를 변환(transform)\n",
    "        X_scaled = scaler.transform(X_features)\n",
    "\n",
    "        # 2. 변환된 NumPy 배열을 다시 DataFrame으로 만듦 (컬럼명 복원)\n",
    "        df_scaled_features = pd.DataFrame(X_scaled, columns=FEATURE_COLUMNS)\n",
    "\n",
    "        # 3. 타겟(tsunami) 열을 다시 결합\n",
    "        df_final_scaled_data = df_scaled_features.copy()\n",
    "        df_final_scaled_data[TARGET_COLUMN] = y_target.values\n",
    "\n",
    "        # 4. 새 CSV 파일로 저장\n",
    "        df_final_scaled_data.to_csv(SCALED_DATA_OUTPUT, index=False)\n",
    "\n",
    "        print(f\"성공! 정형화된 데이터(6-features + target)가 '{SCALED_DATA_OUTPUT}' 파일로 저장되었습니다.\")\n",
    "\n",
    "        print(\"\\n이제 'train_v18_model.py' 스크립트를 실행하여 모델을 훈련할 수 있습니다.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: '{INPUT_CSV}' 파일을 찾을 수 없습니다.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"오류: '{INPUT_CSV}' 파일에 필요한 특성 열이 없습니다: {e}\")\n",
    "        print(f\"필요한 열: {FEATURE_COLUMNS} 또는 '{TARGET_COLUMN}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"작업 중 오류 발생: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_scaler_and_scaled_data()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'final_802_data_for_training.csv' 로드 완료.\n",
      "예측 호환용 6개 특성 선택 완료: ['magnitude', 'depth', 'is_ocean', 'is_steep_slope', 'horizontal_count_1y_full', 'vertical_count_1y_full']\n",
      "StandardScaler 학습(fit) 완료.\n",
      "\n",
      "[작업 1] 성공! 스케일러가 'tsunami_scaler.joblib' 파일로 저장되었습니다.\n",
      "\n",
      "[작업 2] 데이터를 변환하여 'scaled_model_data_6features.csv' 파일로 저장합니다...\n",
      "성공! 정형화된 데이터(6-features + target)가 'scaled_model_data_6features.csv' 파일로 저장되었습니다.\n",
      "\n",
      "이제 'train_v18_model.py' 스크립트를 실행하여 모델을 훈련할 수 있습니다.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T12:24:48.495284Z",
     "start_time": "2025-11-02T12:24:48.457174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def train_model_from_preprocessed(input_file, model_output_file):\n",
    "    \"\"\"\n",
    "    이미 전처리된 데이터를 로드하여 XGBoost 모델을 훈련시키고,\n",
    "    성능을 평가한 뒤 모델을 저장합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. 전처리된 데이터 로드\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\"전처리된 파일 '{input_file}' 로드 완료. (총 {len(df)}개 행)\")\n",
    "\n",
    "        # 2. 피처(X)와 타겟(y) 분리\n",
    "        target_column = 'tsunami'\n",
    "        if target_column not in df.columns:\n",
    "            print(f\"오류: 타겟 컬럼 '{target_column}'을 찾을 수 없습니다.\")\n",
    "            return\n",
    "\n",
    "        X = df.drop(target_column, axis=1)\n",
    "        y = df[target_column]\n",
    "        print(\"피처(X)와 타겟(y) 분리 완료.\")\n",
    "\n",
    "        # 3. 학습 데이터와 테스트 데이터 분리 (80% 학습, 20% 테스트)\n",
    "        # stratify=y : 원본 데이터의 타겟(tsunami) 비율을 유지하며 분리\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,  # 스케일링된 X를 바로 사용\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=y\n",
    "        )\n",
    "        print(f\"데이터 분리 완료: 학습용 {len(X_train)}개, 테스트용 {len(X_test)}개\")\n",
    "\n",
    "        # 4. XGBoost 모델 초기화 및 학습\n",
    "        # scale_pos_weight: 타겟(y)의 0과 1 비율 불균형을 보정합니다.\n",
    "        # (0의 개수 / 1의 개수)\n",
    "        ratio = (y == 0).sum() / (y == 1).sum()\n",
    "\n",
    "        model = xgb.XGBClassifier(\n",
    "            objective='binary:logistic',  # 이진 분류\n",
    "            scale_pos_weight=ratio,       # 클래스 불균형 보정\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            use_label_encoder=False # 최신 XGBoost 호환성\n",
    "        )\n",
    "\n",
    "        print(\"\\nXGBoost 모델 학습 시작...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"모델 학습 완료.\")\n",
    "\n",
    "        # 5. 모델 성능 평가 (테스트 데이터)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # 정확도 (Accuracy)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # 정밀도 (Precision): 모델이 '쓰나미 발생(1)'이라고 예측한 것 중, '실제 쓰나미(1)'인 비율\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "\n",
    "        print(\"\\n--- 모델 성능 평가 (Test Set) ---\")\n",
    "        print(f\"정확도 (Accuracy):   {accuracy * 100:.2f} %\")\n",
    "        print(f\"정밀도 (Precision): {precision * 100:.2f} %\")\n",
    "        print(\"---------------------------------\")\n",
    "\n",
    "        # 6. 훈련된 모델 파일로 저장\n",
    "        joblib.dump(model, model_output_file)\n",
    "        print(f\"\\n훈련된 XGBoost 모델 저장 완료: '{model_output_file}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: '{input_file}' 파일을 찾을 수 없습니다.\")\n",
    "        print(\"이 스크립트는 'scaled_final_model_data.csv' 파일이 필요합니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"작업 중 오류 발생: {e}\")\n",
    "\n",
    "# --- 스크립트 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # [중요] 입력 파일로 'scaled_final_model_data.csv'을 사용합니다.\n",
    "    INPUT_CSV = 'scaled_model_data_6features.csv'\n",
    "    MODEL_OUTPUT = 'xgboost_tsunami_model.joblib'\n",
    "\n",
    "    train_model_from_preprocessed(INPUT_CSV, MODEL_OUTPUT)"
   ],
   "id": "a528b4964db502c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 파일 'scaled_model_data_6features.csv' 로드 완료. (총 802개 행)\n",
      "피처(X)와 타겟(y) 분리 완료.\n",
      "데이터 분리 완료: 학습용 641개, 테스트용 161개\n",
      "\n",
      "XGBoost 모델 학습 시작...\n",
      "모델 학습 완료.\n",
      "\n",
      "--- 모델 성능 평가 (Test Set) ---\n",
      "정확도 (Accuracy):   60.25 %\n",
      "정밀도 (Precision): 50.82 %\n",
      "---------------------------------\n",
      "\n",
      "훈련된 XGBoost 모델 저장 완료: 'xgboost_tsunami_model.joblib'\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T11:31:54.064693Z",
     "start_time": "2025-11-02T11:31:54.057073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# 1. 원본 데이터 파일\n",
    "INPUT_CSV = 'final_model_dataset.csv'\n",
    "\n",
    "# 2. ★★★ 예측 스크립트와 호환되는 6개 특성 ★★★\n",
    "# (예측 스크립트의 X_new = np.array([...]) 순서와 정확히 일치해야 함)\n",
    "FEATURE_COLUMNS = [\n",
    "    'magnitude',\n",
    "    'depth',\n",
    "    'is_ocean',\n",
    "    'is_steep_slope',\n",
    "    'horizontal_count_1y_full', # (h_count)\n",
    "    'vertical_count_1y_full'    # (v_count)\n",
    "]\n",
    "\n",
    "# 3. 예측 스크립트가 요구하는 스케일러 파일 이름\n",
    "SCALER_OUTPUT = 'tsunami_scaler.joblib'\n",
    "\n",
    "def create_compatible_scaler():\n",
    "    \"\"\"\n",
    "    예측 스크립트(V18)와 호환되는 6-feature 스케일러를 생성하고 저장합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 원본 데이터 로드\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "        print(f\"'{INPUT_CSV}' 로드 완료.\")\n",
    "\n",
    "        # 6개 특성만 선택\n",
    "        X_features = df[FEATURE_COLUMNS]\n",
    "        print(f\"예측 호환용 6개 특성 선택 완료: {FEATURE_COLUMNS}\")\n",
    "\n",
    "        # StandardScaler 생성 및 학습(fit)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_features)\n",
    "\n",
    "        print(\"StandardScaler 학습(fit) 완료.\")\n",
    "\n",
    "        # 학습된 스케일러 객체를 파일로 저장\n",
    "        joblib.dump(scaler, SCALER_OUTPUT)\n",
    "\n",
    "        print(f\"\\n성공! '{SCALER_OUTPUT}' 파일이 저장되었습니다.\")\n",
    "        print(\"이제 'train_v18_model.py' 스크립트를 실행하세요.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: '{INPUT_CSV}' 파일을 찾을 수 없습니다.\")\n",
    "    except KeyError:\n",
    "        print(f\"오류: '{INPUT_CSV}' 파일에 필요한 특성 열이 없습니다.\")\n",
    "        print(f\"필요한 열: {FEATURE_COLUMNS}\")\n",
    "    except Exception as e:\n",
    "        print(f\"작업 중 오류 발생: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_compatible_scaler()"
   ],
   "id": "d1683ac916dc2385",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'final_model_dataset.csv' 로드 완료.\n",
      "예측 호환용 6개 특성 선택 완료: ['magnitude', 'depth', 'is_ocean', 'is_steep_slope', 'horizontal_count_1y_full', 'vertical_count_1y_full']\n",
      "StandardScaler 학습(fit) 완료.\n",
      "\n",
      "성공! 'tsunami_scaler.joblib' 파일이 저장되었습니다.\n",
      "이제 'train_v18_model.py' 스크립트를 실행하세요.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T12:08:34.437149Z",
     "start_time": "2025-11-02T12:08:34.428061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 설정 ---\n",
    "INPUT_FILE = 'final_model_data.csv'\n",
    "OUTPUT_FILE = 'final_model_data_6features.csv'\n",
    "\n",
    "# 제거할 컬럼 목록\n",
    "COLUMNS_TO_DROP = [\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'Year',\n",
    "    'Month'\n",
    "]\n",
    "\n",
    "# (참고) 최종적으로 남게 될 컬럼 목록\n",
    "# Features: 'magnitude', 'depth', 'is_ocean', 'is_steep_slope',\n",
    "#           'horizontal_count_1y_full', 'vertical_count_1y_full'\n",
    "# Target: 'tsunami'\n",
    "\n",
    "def extract_6_features(input_file, output_file, columns_to_drop):\n",
    "    \"\"\"\n",
    "    원본 데이터 파일에서 지정된 컬럼들을 제거하고 새 파일로 저장합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. 원본 데이터 로드\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\"'{input_file}' 로드 완료. (원본 컬럼: {list(df.columns)})\")\n",
    "\n",
    "        # 2. 지정된 컬럼 제거\n",
    "        # df.drop()은 컬럼이 없는 경우 오류를 발생시킬 수 있으므로,\n",
    "        # 현재 데이터프레임에 '있는' 컬럼만 제거하도록 처리합니다.\n",
    "\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "        if not existing_columns_to_drop:\n",
    "            print(f\"제거할 컬럼이 '{input_file}'에 존재하지 않습니다.\")\n",
    "            return\n",
    "\n",
    "        df_reduced = df.drop(columns=existing_columns_to_drop)\n",
    "\n",
    "        print(f\"컬럼 제거 완료: {existing_columns_to_drop}\")\n",
    "\n",
    "        # 3. 새 파일로 저장\n",
    "        df_reduced.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"\\n성공! 6-feature 데이터셋이 '{output_file}'로 저장되었습니다.\")\n",
    "        print(f\"저장된 컬럼: {list(df_reduced.columns)}\")\n",
    "        print(\"\\n이제 이 파일을 사용하여 '스케일러 생성' 및 '모델 훈련'을 진행할 수 있습니다.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: '{input_file}' 파일을 찾을 수 없습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"작업 중 오류 발생: {e}\")\n",
    "\n",
    "# --- 스크립트 실행 ---\n",
    "if __name__ == \"__main__\":\n",
    "    extract_6_features(INPUT_FILE, OUTPUT_FILE, COLUMNS_TO_DROP)"
   ],
   "id": "f2e681e1ee9e67f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'final_model_data.csv' 로드 완료. (원본 컬럼: ['magnitude', 'depth', 'latitude', 'longitude', 'Year', 'Month', 'tsunami', 'is_ocean', 'is_steep_slope', 'horizontal_count_1y_full', 'vertical_count_1y_full'])\n",
      "컬럼 제거 완료: ['latitude', 'longitude', 'Year', 'Month']\n",
      "\n",
      "성공! 6-feature 데이터셋이 'final_model_data_6features.csv'로 저장되었습니다.\n",
      "저장된 컬럼: ['magnitude', 'depth', 'tsunami', 'is_ocean', 'is_steep_slope', 'horizontal_count_1y_full', 'vertical_count_1y_full']\n",
      "\n",
      "이제 이 파일을 사용하여 '스케일러 생성' 및 '모델 훈련'을 진행할 수 있습니다.\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
