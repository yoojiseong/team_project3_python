{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-03T07:37:17.631543Z",
     "start_time": "2025-11-03T07:37:17.434517Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# 병합할 파일 목록 (파일 이름 패턴을 사용하거나, 리스트로 직접 지정)\n",
    "# 4개의 파일 이름이 query (숫자).csv 형태이므로 glob을 사용하면 편리합니다.\n",
    "file_list = ['query (7).csv', 'query (6).csv', 'query (5).csv', 'query (4).csv']\n",
    "\n",
    "# 또는 glob.glob(\"query (*).csv\") 를 사용할 수도 있습니다.\n",
    "\n",
    "# 유지할 컬럼 목록\n",
    "columns_to_keep = ['time', 'latitude', 'longitude', 'depth', 'nst', 'gap', 'mag']\n",
    "\n",
    "# 각 파일을 읽어와서 리스트에 저장\n",
    "all_dataframes = []\n",
    "for filename in file_list:\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # 파일에 필요한 컬럼이 모두 있는지 확인\n",
    "        if all(col in df.columns for col in columns_to_keep):\n",
    "            # 필요한 컬럼만 선택\n",
    "            df_selected = df[columns_to_keep]\n",
    "            all_dataframes.append(df_selected)\n",
    "            print(f\"'{filename}' 처리 완료 (행: {len(df_selected)})\")\n",
    "        else:\n",
    "            print(f\"경고: '{filename}'에 필요한 컬럼이 부족하여 건너뜁니다.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"'{filename}' 처리 중 오류 발생: {e}\")\n",
    "\n",
    "# 모든 데이터프레임을 하나로 합치기 (위아래로 붙이기)\n",
    "if all_dataframes:\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # 결과 파일로 저장\n",
    "    output_filename = 'combined_earthquake_data.csv'\n",
    "    combined_df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n총 {len(combined_df)}개의 행이 '{output_filename}' 파일로 저장되었습니다.\")\n",
    "else:\n",
    "    print(\"\\n병합할 데이터가 없습니다. 파일 이름이나 컬럼명을 확인해주세요.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'query (7).csv' 처리 완료 (행: 6768)\n",
      "'query (6).csv' 처리 완료 (행: 6323)\n",
      "'query (5).csv' 처리 완료 (행: 5393)\n",
      "'query (4).csv' 처리 완료 (행: 4023)\n",
      "\n",
      "총 22507개의 행이 'combined_earthquake_data.csv' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:51:31.709544Z",
     "start_time": "2025-11-03T07:51:31.689635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 입력 파일 (TSV)\n",
    "input_file = 'tsunamis-2025-11-03_13-35-46_+0900.tsv'\n",
    "\n",
    "# 출력 파일 (CSV)\n",
    "output_file = 'renamed_tsunami_data.csv'\n",
    "\n",
    "# 변경할 컬럼 이름 맵핑\n",
    "# { '기존 컬럼명' : '새 컬럼명' }\n",
    "rename_map = {\n",
    "    'Earthquake Magnitude': 'mag',\n",
    "    'Tsunami Cause Code': 'tsunami',\n",
    "    'Latitude': 'latitude',\n",
    "    'Longitude': 'longitude'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 1. TSV 파일을 읽어옵니다. (구분자=탭)\n",
    "    #    (파일 구조 확인 후 header=0, skiprows=[1] 사용)\n",
    "    df = pd.read_csv(input_file, sep='\\t', header=0, skiprows=[1])\n",
    "\n",
    "    # 컬럼 이름 앞뒤에 붙어있을 수 있는 따옴표 제거\n",
    "    df.columns = df.columns.str.strip('\"')\n",
    "\n",
    "    # 2. 지정된 컬럼명 변경\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # 3. 변경된 DataFrame을 새 CSV 파일로 저장 (기본값=쉼표 구분)\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"파일 처리 완료! '{output_file}' (CSV 파일)로 저장되었습니다.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}'을(를) 찾을 수 없습니다. 파일이 코드와 같은 위치에 있는지 확인하세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ],
   "id": "92a8c1b5f4c25b18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 처리 완료! 'renamed_tsunami_data.csv' (CSV 파일)로 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:56:14.471285Z",
     "start_time": "2025-11-03T07:56:14.447761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 입력 파일\n",
    "input_file = 'renamed_tsunami_data.csv'\n",
    "# 저장할 파일\n",
    "output_file = 'filtered_tsunami_data.csv'\n",
    "\n",
    "# 1. 유지할 컬럼 목록\n",
    "columns_to_keep = [\n",
    "    'Year',\n",
    "    'Mo',\n",
    "    'tsunami',\n",
    "    'mag',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'Tsunami Event Validity'\n",
    "]\n",
    "\n",
    "# 2. 필터링할 컬럼과 값\n",
    "filter_column = 'Tsunami Event Validity'\n",
    "valid_values = [3, 4] # 3 또는 4 인 값만 유지\n",
    "\n",
    "try:\n",
    "    # 파일 읽기\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # 3. 지정된 컬럼만 선택 (유지)\n",
    "    # 리스트에 없는 모든 컬럼은 삭제됩니다.\n",
    "    df_selected = df[columns_to_keep]\n",
    "\n",
    "    # 4. 'Tsunami Event Validity' 컬럼 값이 3 또는 4인 행만 필터링\n",
    "    # .isin() 함수를 사용하여 [3, 4] 리스트에 포함된 값만 확인\n",
    "    df_filtered = df_selected[df_selected[filter_column].isin(valid_values)]\n",
    "\n",
    "    # 5. 새 CSV 파일로 저장\n",
    "    df_filtered.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"처리 완료! '{output_file}'로 저장되었습니다.\")\n",
    "    print(f\"원본 행: {len(df)}, 컬럼 선택 후: {len(df_selected)}, 최종 필터링 후: {len(df_filtered)}\")\n",
    "    print(\"\\n필터링된 데이터 (상위 5개):\")\n",
    "    print(df_filtered.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}'을(를) 찾을 수 없습니다.\")\n",
    "except KeyError as e:\n",
    "    # columns_to_keep 리스트에 없는 컬럼명을 요청했을 때 발생\n",
    "    print(f\"오류: {e} 컬럼을 찾을 수 없습니다. 컬럼명을 확인하세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"알 수 없는 오류가 발생했습니다: {e}\")"
   ],
   "id": "f58adceab373c104",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 완료! 'filtered_tsunami_data.csv'로 저장되었습니다.\n",
      "원본 행: 135, 컬럼 선택 후: 135, 최종 필터링 후: 130\n",
      "\n",
      "필터링된 데이터 (상위 5개):\n",
      "   Year  Mo  tsunami  mag  latitude  longitude  Tsunami Event Validity\n",
      "0  2001   1        1  7.7    13.049    -88.660                       3\n",
      "1  2001   6        1  8.4   -16.265    -73.641                       4\n",
      "2  2001  10        1  6.1    52.630   -132.200                       4\n",
      "3  2001  12        1  6.8    23.954    122.734                       4\n",
      "4  2002   1        1  7.2   -17.600    167.856                       4\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:57:35.067971Z",
     "start_time": "2025-11-03T07:57:35.057006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 입력 파일\n",
    "input_file = 'filtered_tsunami_data.csv'\n",
    "# 새 출력 파일\n",
    "output_file = 'final_tsunami_data.csv'\n",
    "\n",
    "try:\n",
    "    # 1. CSV 파일 읽기\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # 2. 'Mo' 컬럼을 'Month'로 이름 변경\n",
    "    df.rename(columns={'Mo': 'Month'}, inplace=True)\n",
    "\n",
    "    # 3. 새 CSV 파일로 저장\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"컬럼명 변경 완료! '{output_file}' 로 저장되었습니다.\")\n",
    "    print(\"\\n변경된 데이터 확인 (상위 5개):\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}'을(를) 찾을 수 없습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ],
   "id": "d2cfc29730e0be27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컬럼명 변경 완료! 'final_tsunami_data.csv' 로 저장되었습니다.\n",
      "\n",
      "변경된 데이터 확인 (상위 5개):\n",
      "   Year  Month  tsunami  mag  latitude  longitude  Tsunami Event Validity\n",
      "0  2001      1        1  7.7    13.049    -88.660                       3\n",
      "1  2001      6        1  8.4   -16.265    -73.641                       4\n",
      "2  2001     10        1  6.1    52.630   -132.200                       4\n",
      "3  2001     12        1  6.8    23.954    122.734                       4\n",
      "4  2002      1        1  7.2   -17.600    167.856                       4\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T08:00:26.484070Z",
     "start_time": "2025-11-03T08:00:26.473790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 입력 파일\n",
    "input_file = 'final_tsunami_data.csv'\n",
    "# 새 출력 파일\n",
    "output_file = 'real_final_tsunami_data.csv'\n",
    "# 삭제할 컬럼 이름\n",
    "column_to_drop = 'Tsunami Event Validity'\n",
    "\n",
    "try:\n",
    "    # 1. CSV 파일 읽기\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # 2. 'time' 컬럼이 있는지 확인\n",
    "    if column_to_drop in df.columns:\n",
    "        # 3. 'time' 컬럼 삭제\n",
    "        # df.drop()을 사용하고 axis=1 (컬럼 기준)을 지정합니다.\n",
    "        df_dropped = df.drop(columns=[column_to_drop])\n",
    "\n",
    "        # 4. 새 CSV 파일로 저장\n",
    "        df_dropped.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        print(f\"'{column_to_drop}' 컬럼을 성공적으로 삭제했습니다.\")\n",
    "        print(f\"새 파일: '{output_file}'\")\n",
    "        print(\"\\n삭제 후 컬럼 목록:\")\n",
    "        print(df_dropped.columns.to_list())\n",
    "\n",
    "    else:\n",
    "        print(f\"오류: '{column_to_drop}' 컬럼이 파일에 없습니다.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}' 파일을 찾을 수 없습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ],
   "id": "66a6127c74bfac46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Tsunami Event Validity' 컬럼을 성공적으로 삭제했습니다.\n",
      "새 파일: 'real_final_tsunami_data.csv'\n",
      "\n",
      "삭제 후 컬럼 목록:\n",
      "['Year', 'Month', 'tsunami', 'mag', 'latitude', 'longitude']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T08:10:49.462670Z",
     "start_time": "2025-11-03T08:10:49.402844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 파일 설정 ---\n",
    "# 비교할 두 파일\n",
    "file1_earthquake = 'combined_earthquake_data.csv'\n",
    "file2_tsunami = 'real_final_tsunami_data.csv'\n",
    "\n",
    "# 결과(공통 행)를 저장할 파일\n",
    "output_file = 'common_events_matched.csv'\n",
    "\n",
    "# 비교할 기준이 되는 컬럼들\n",
    "merge_columns = ['mag', 'latitude', 'longitude']\n",
    "\n",
    "# 부동소수점(소수점) 비교를 위한 반올림 자릿수\n",
    "# (예: 5.12와 5.120을 같게 처리하기 위함)\n",
    "precision = 3\n",
    "# --- --- ---\n",
    "\n",
    "try:\n",
    "    # 1. 두 CSV 파일 읽기\n",
    "    df_eq = pd.read_csv(file1_earthquake)\n",
    "    df_tsu = pd.read_csv(file2_tsunami)\n",
    "\n",
    "    print(f\"'{file1_earthquake}' 파일 로드 (총 {len(df_eq)} 행)\")\n",
    "    print(f\"'{file2_tsunami}' 파일 로드 (총 {len(df_tsu)} 행)\")\n",
    "\n",
    "    # 2. 두 파일에 필요한 컬럼이 모두 있는지 확인\n",
    "    missing_eq = [col for col in merge_columns if col not in df_eq.columns]\n",
    "    missing_tsu = [col for col in merge_columns if col not in df_tsu.columns]\n",
    "\n",
    "    error_found = False\n",
    "    if missing_eq:\n",
    "        print(f\"오류: '{file1_earthquake}' 파일에 다음 컬럼이 없습니다: {missing_eq}\")\n",
    "        error_found = True\n",
    "    if missing_tsu:\n",
    "        print(f\"오류: '{file2_tsunami}' 파일에 다음 컬럼이 없습니다: {missing_tsu}\")\n",
    "        error_found = True\n",
    "\n",
    "    if not error_found:\n",
    "        # 3. 비교 전 데이터 처리 (부동소수점 오차 방지)\n",
    "        # 'mag', 'latitude', 'longitude' 컬럼을 숫자형으로 변환하고\n",
    "        # 지정된 자릿수(3)에서 반올림하여 정확한 비교가 가능하게 함\n",
    "        print(f\"\\n비교를 위해 기준 컬럼 {merge_columns} 값을 소수점 {precision}째 자리까지 표준화합니다...\")\n",
    "\n",
    "        # 원본을 유지하기 위해 .copy() 사용\n",
    "        df_eq_processed = df_eq.copy()\n",
    "        df_tsu_processed = df_tsu.copy()\n",
    "\n",
    "        for col in merge_columns:\n",
    "            # errors='coerce'는 숫자로 변환할 수 없는 값을 NaN(결측치)로 만듭니다.\n",
    "            df_eq_processed[col] = pd.to_numeric(df_eq_processed[col], errors='coerce').round(precision)\n",
    "            df_tsu_processed[col] = pd.to_numeric(df_tsu_processed[col], errors='coerce').round(precision)\n",
    "\n",
    "        # 비교 컬럼 중 하나라도 NaN인 행은 삭제\n",
    "        df_eq_processed.dropna(subset=merge_columns, inplace=True)\n",
    "        df_tsu_processed.dropna(subset=merge_columns, inplace=True)\n",
    "\n",
    "        # 4. 'inner' 조인(merge) 수행\n",
    "        # 'how='inner'' 옵션은 두 데이터프레임에서\n",
    "        # 'on=merge_columns' (mag, latitude, longitude) 값이 모두 일치하는 행만 남깁니다.\n",
    "        # (이것이 SQL의 INNER JOIN과 동일한 기능입니다)\n",
    "        common_events_df = pd.merge(\n",
    "            df_eq_processed,\n",
    "            df_tsu_processed,\n",
    "            on=merge_columns,\n",
    "            how='inner',\n",
    "            suffixes=('_eq', '_tsu') # 두 파일에 이름이 겹치는 다른 컬럼(예: depth)을 구분하기 위함\n",
    "        )\n",
    "\n",
    "        # 5. 결과 저장 및 보고\n",
    "        if len(common_events_df) > 0:\n",
    "            common_events_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n처리 완료! {len(common_events_df)} 개의 공통 행을 찾았습니다.\")\n",
    "            print(f\"'{output_file}' 파일로 저장되었습니다.\")\n",
    "        else:\n",
    "            print(f\"\\n두 파일 간에 {merge_columns} 값이 모두 일치하는 공통 행을 찾지 못했습니다.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"오류: 파일을 찾을 수 없습니다. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"알 수 없는 오류가 발생했습니다: {e}\")"
   ],
   "id": "6bee0000580b0b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'combined_earthquake_data.csv' 파일 로드 (총 22507 행)\n",
      "'real_final_tsunami_data.csv' 파일 로드 (총 130 행)\n",
      "\n",
      "비교를 위해 기준 컬럼 ['mag', 'latitude', 'longitude'] 값을 소수점 3째 자리까지 표준화합니다...\n",
      "\n",
      "처리 완료! 91 개의 공통 행을 찾았습니다.\n",
      "'common_events_matched.csv' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
