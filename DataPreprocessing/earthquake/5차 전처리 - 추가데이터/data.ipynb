{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-03T07:37:17.631543Z",
     "start_time": "2025-11-03T07:37:17.434517Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# 병합할 파일 목록 (파일 이름 패턴을 사용하거나, 리스트로 직접 지정)\n",
    "# 4개의 파일 이름이 query (숫자).csv 형태이므로 glob을 사용하면 편리합니다.\n",
    "file_list = ['query (7).csv', 'query (6).csv', 'query (5).csv', 'query (4).csv']\n",
    "\n",
    "# 또는 glob.glob(\"query (*).csv\") 를 사용할 수도 있습니다.\n",
    "\n",
    "# 유지할 컬럼 목록\n",
    "columns_to_keep = ['time', 'latitude', 'longitude', 'depth', 'nst', 'gap', 'mag']\n",
    "\n",
    "# 각 파일을 읽어와서 리스트에 저장\n",
    "all_dataframes = []\n",
    "for filename in file_list:\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # 파일에 필요한 컬럼이 모두 있는지 확인\n",
    "        if all(col in df.columns for col in columns_to_keep):\n",
    "            # 필요한 컬럼만 선택\n",
    "            df_selected = df[columns_to_keep]\n",
    "            all_dataframes.append(df_selected)\n",
    "            print(f\"'{filename}' 처리 완료 (행: {len(df_selected)})\")\n",
    "        else:\n",
    "            print(f\"경고: '{filename}'에 필요한 컬럼이 부족하여 건너뜁니다.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"'{filename}' 처리 중 오류 발생: {e}\")\n",
    "\n",
    "# 모든 데이터프레임을 하나로 합치기 (위아래로 붙이기)\n",
    "if all_dataframes:\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # 결과 파일로 저장\n",
    "    output_filename = 'combined_earthquake_data.csv'\n",
    "    combined_df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n총 {len(combined_df)}개의 행이 '{output_filename}' 파일로 저장되었습니다.\")\n",
    "else:\n",
    "    print(\"\\n병합할 데이터가 없습니다. 파일 이름이나 컬럼명을 확인해주세요.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'query (7).csv' 처리 완료 (행: 6768)\n",
      "'query (6).csv' 처리 완료 (행: 6323)\n",
      "'query (5).csv' 처리 완료 (행: 5393)\n",
      "'query (4).csv' 처리 완료 (행: 4023)\n",
      "\n",
      "총 22507개의 행이 'combined_earthquake_data.csv' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:51:31.709544Z",
     "start_time": "2025-11-03T07:51:31.689635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 입력 파일 (TSV)\n",
    "input_file = 'tsunamis-2025-11-03_13-35-46_+0900.tsv'\n",
    "\n",
    "# 출력 파일 (CSV)\n",
    "output_file = 'renamed_tsunami_data.csv'\n",
    "\n",
    "# 변경할 컬럼 이름 맵핑\n",
    "# { '기존 컬럼명' : '새 컬럼명' }\n",
    "rename_map = {\n",
    "    'Earthquake Magnitude': 'mag',\n",
    "    'Tsunami Cause Code': 'tsunami',\n",
    "    'Latitude': 'latitude',\n",
    "    'Longitude': 'longitude'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 1. TSV 파일을 읽어옵니다. (구분자=탭)\n",
    "    #    (파일 구조 확인 후 header=0, skiprows=[1] 사용)\n",
    "    df = pd.read_csv(input_file, sep='\\t', header=0, skiprows=[1])\n",
    "\n",
    "    # 컬럼 이름 앞뒤에 붙어있을 수 있는 따옴표 제거\n",
    "    df.columns = df.columns.str.strip('\"')\n",
    "\n",
    "    # 2. 지정된 컬럼명 변경\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # 3. 변경된 DataFrame을 새 CSV 파일로 저장 (기본값=쉼표 구분)\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"파일 처리 완료! '{output_file}' (CSV 파일)로 저장되었습니다.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}'을(를) 찾을 수 없습니다. 파일이 코드와 같은 위치에 있는지 확인하세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ],
   "id": "92a8c1b5f4c25b18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 처리 완료! 'renamed_tsunami_data.csv' (CSV 파일)로 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:56:14.471285Z",
     "start_time": "2025-11-03T07:56:14.447761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 입력 파일\n",
    "input_file = 'renamed_tsunami_data.csv'\n",
    "# 저장할 파일\n",
    "output_file = 'filtered_tsunami_data.csv'\n",
    "\n",
    "# 1. 유지할 컬럼 목록\n",
    "columns_to_keep = [\n",
    "    'Year',\n",
    "    'Mo',\n",
    "    'tsunami',\n",
    "    'mag',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'Tsunami Event Validity'\n",
    "]\n",
    "\n",
    "# 2. 필터링할 컬럼과 값\n",
    "filter_column = 'Tsunami Event Validity'\n",
    "valid_values = [3, 4] # 3 또는 4 인 값만 유지\n",
    "\n",
    "try:\n",
    "    # 파일 읽기\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # 3. 지정된 컬럼만 선택 (유지)\n",
    "    # 리스트에 없는 모든 컬럼은 삭제됩니다.\n",
    "    df_selected = df[columns_to_keep]\n",
    "\n",
    "    # 4. 'Tsunami Event Validity' 컬럼 값이 3 또는 4인 행만 필터링\n",
    "    # .isin() 함수를 사용하여 [3, 4] 리스트에 포함된 값만 확인\n",
    "    df_filtered = df_selected[df_selected[filter_column].isin(valid_values)]\n",
    "\n",
    "    # 5. 새 CSV 파일로 저장\n",
    "    df_filtered.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"처리 완료! '{output_file}'로 저장되었습니다.\")\n",
    "    print(f\"원본 행: {len(df)}, 컬럼 선택 후: {len(df_selected)}, 최종 필터링 후: {len(df_filtered)}\")\n",
    "    print(\"\\n필터링된 데이터 (상위 5개):\")\n",
    "    print(df_filtered.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}'을(를) 찾을 수 없습니다.\")\n",
    "except KeyError as e:\n",
    "    # columns_to_keep 리스트에 없는 컬럼명을 요청했을 때 발생\n",
    "    print(f\"오류: {e} 컬럼을 찾을 수 없습니다. 컬럼명을 확인하세요.\")\n",
    "except Exception as e:\n",
    "    print(f\"알 수 없는 오류가 발생했습니다: {e}\")"
   ],
   "id": "f58adceab373c104",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 완료! 'filtered_tsunami_data.csv'로 저장되었습니다.\n",
      "원본 행: 135, 컬럼 선택 후: 135, 최종 필터링 후: 130\n",
      "\n",
      "필터링된 데이터 (상위 5개):\n",
      "   Year  Mo  tsunami  mag  latitude  longitude  Tsunami Event Validity\n",
      "0  2001   1        1  7.7    13.049    -88.660                       3\n",
      "1  2001   6        1  8.4   -16.265    -73.641                       4\n",
      "2  2001  10        1  6.1    52.630   -132.200                       4\n",
      "3  2001  12        1  6.8    23.954    122.734                       4\n",
      "4  2002   1        1  7.2   -17.600    167.856                       4\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:57:35.067971Z",
     "start_time": "2025-11-03T07:57:35.057006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 입력 파일\n",
    "input_file = 'filtered_tsunami_data.csv'\n",
    "# 새 출력 파일\n",
    "output_file = 'final_tsunami_data.csv'\n",
    "\n",
    "try:\n",
    "    # 1. CSV 파일 읽기\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # 2. 'Mo' 컬럼을 'Month'로 이름 변경\n",
    "    df.rename(columns={'Mo': 'Month'}, inplace=True)\n",
    "\n",
    "    # 3. 새 CSV 파일로 저장\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"컬럼명 변경 완료! '{output_file}' 로 저장되었습니다.\")\n",
    "    print(\"\\n변경된 데이터 확인 (상위 5개):\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}'을(를) 찾을 수 없습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ],
   "id": "d2cfc29730e0be27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컬럼명 변경 완료! 'final_tsunami_data.csv' 로 저장되었습니다.\n",
      "\n",
      "변경된 데이터 확인 (상위 5개):\n",
      "   Year  Month  tsunami  mag  latitude  longitude  Tsunami Event Validity\n",
      "0  2001      1        1  7.7    13.049    -88.660                       3\n",
      "1  2001      6        1  8.4   -16.265    -73.641                       4\n",
      "2  2001     10        1  6.1    52.630   -132.200                       4\n",
      "3  2001     12        1  6.8    23.954    122.734                       4\n",
      "4  2002      1        1  7.2   -17.600    167.856                       4\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T08:00:26.484070Z",
     "start_time": "2025-11-03T08:00:26.473790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 입력 파일\n",
    "input_file = 'final_tsunami_data.csv'\n",
    "# 새 출력 파일\n",
    "output_file = 'real_final_tsunami_data.csv'\n",
    "# 삭제할 컬럼 이름\n",
    "column_to_drop = 'Tsunami Event Validity'\n",
    "\n",
    "try:\n",
    "    # 1. CSV 파일 읽기\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # 2. 'time' 컬럼이 있는지 확인\n",
    "    if column_to_drop in df.columns:\n",
    "        # 3. 'time' 컬럼 삭제\n",
    "        # df.drop()을 사용하고 axis=1 (컬럼 기준)을 지정합니다.\n",
    "        df_dropped = df.drop(columns=[column_to_drop])\n",
    "\n",
    "        # 4. 새 CSV 파일로 저장\n",
    "        df_dropped.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        print(f\"'{column_to_drop}' 컬럼을 성공적으로 삭제했습니다.\")\n",
    "        print(f\"새 파일: '{output_file}'\")\n",
    "        print(\"\\n삭제 후 컬럼 목록:\")\n",
    "        print(df_dropped.columns.to_list())\n",
    "\n",
    "    else:\n",
    "        print(f\"오류: '{column_to_drop}' 컬럼이 파일에 없습니다.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}' 파일을 찾을 수 없습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ],
   "id": "66a6127c74bfac46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Tsunami Event Validity' 컬럼을 성공적으로 삭제했습니다.\n",
      "새 파일: 'real_final_tsunami_data.csv'\n",
      "\n",
      "삭제 후 컬럼 목록:\n",
      "['Year', 'Month', 'tsunami', 'mag', 'latitude', 'longitude']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T08:10:49.462670Z",
     "start_time": "2025-11-03T08:10:49.402844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 파일 설정 ---\n",
    "# 비교할 두 파일\n",
    "file1_earthquake = 'combined_earthquake_data.csv'\n",
    "file2_tsunami = 'real_final_tsunami_data.csv'\n",
    "\n",
    "# 결과(공통 행)를 저장할 파일\n",
    "output_file = 'common_events_matched.csv'\n",
    "\n",
    "# 비교할 기준이 되는 컬럼들\n",
    "merge_columns = ['mag', 'latitude', 'longitude']\n",
    "\n",
    "# 부동소수점(소수점) 비교를 위한 반올림 자릿수\n",
    "# (예: 5.12와 5.120을 같게 처리하기 위함)\n",
    "precision = 3\n",
    "# --- --- ---\n",
    "\n",
    "try:\n",
    "    # 1. 두 CSV 파일 읽기\n",
    "    df_eq = pd.read_csv(file1_earthquake)\n",
    "    df_tsu = pd.read_csv(file2_tsunami)\n",
    "\n",
    "    print(f\"'{file1_earthquake}' 파일 로드 (총 {len(df_eq)} 행)\")\n",
    "    print(f\"'{file2_tsunami}' 파일 로드 (총 {len(df_tsu)} 행)\")\n",
    "\n",
    "    # 2. 두 파일에 필요한 컬럼이 모두 있는지 확인\n",
    "    missing_eq = [col for col in merge_columns if col not in df_eq.columns]\n",
    "    missing_tsu = [col for col in merge_columns if col not in df_tsu.columns]\n",
    "\n",
    "    error_found = False\n",
    "    if missing_eq:\n",
    "        print(f\"오류: '{file1_earthquake}' 파일에 다음 컬럼이 없습니다: {missing_eq}\")\n",
    "        error_found = True\n",
    "    if missing_tsu:\n",
    "        print(f\"오류: '{file2_tsunami}' 파일에 다음 컬럼이 없습니다: {missing_tsu}\")\n",
    "        error_found = True\n",
    "\n",
    "    if not error_found:\n",
    "        # 3. 비교 전 데이터 처리 (부동소수점 오차 방지)\n",
    "        # 'mag', 'latitude', 'longitude' 컬럼을 숫자형으로 변환하고\n",
    "        # 지정된 자릿수(3)에서 반올림하여 정확한 비교가 가능하게 함\n",
    "        print(f\"\\n비교를 위해 기준 컬럼 {merge_columns} 값을 소수점 {precision}째 자리까지 표준화합니다...\")\n",
    "\n",
    "        # 원본을 유지하기 위해 .copy() 사용\n",
    "        df_eq_processed = df_eq.copy()\n",
    "        df_tsu_processed = df_tsu.copy()\n",
    "\n",
    "        for col in merge_columns:\n",
    "            # errors='coerce'는 숫자로 변환할 수 없는 값을 NaN(결측치)로 만듭니다.\n",
    "            df_eq_processed[col] = pd.to_numeric(df_eq_processed[col], errors='coerce').round(precision)\n",
    "            df_tsu_processed[col] = pd.to_numeric(df_tsu_processed[col], errors='coerce').round(precision)\n",
    "\n",
    "        # 비교 컬럼 중 하나라도 NaN인 행은 삭제\n",
    "        df_eq_processed.dropna(subset=merge_columns, inplace=True)\n",
    "        df_tsu_processed.dropna(subset=merge_columns, inplace=True)\n",
    "\n",
    "        # 4. 'inner' 조인(merge) 수행\n",
    "        # 'how='inner'' 옵션은 두 데이터프레임에서\n",
    "        # 'on=merge_columns' (mag, latitude, longitude) 값이 모두 일치하는 행만 남깁니다.\n",
    "        # (이것이 SQL의 INNER JOIN과 동일한 기능입니다)\n",
    "        common_events_df = pd.merge(\n",
    "            df_eq_processed,\n",
    "            df_tsu_processed,\n",
    "            on=merge_columns,\n",
    "            how='inner',\n",
    "            suffixes=('_eq', '_tsu') # 두 파일에 이름이 겹치는 다른 컬럼(예: depth)을 구분하기 위함\n",
    "        )\n",
    "\n",
    "        # 5. 결과 저장 및 보고\n",
    "        if len(common_events_df) > 0:\n",
    "            common_events_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n처리 완료! {len(common_events_df)} 개의 공통 행을 찾았습니다.\")\n",
    "            print(f\"'{output_file}' 파일로 저장되었습니다.\")\n",
    "        else:\n",
    "            print(f\"\\n두 파일 간에 {merge_columns} 값이 모두 일치하는 공통 행을 찾지 못했습니다.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"오류: 파일을 찾을 수 없습니다. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"알 수 없는 오류가 발생했습니다: {e}\")"
   ],
   "id": "6bee0000580b0b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'combined_earthquake_data.csv' 파일 로드 (총 22507 행)\n",
      "'real_final_tsunami_data.csv' 파일 로드 (총 130 행)\n",
      "\n",
      "비교를 위해 기준 컬럼 ['mag', 'latitude', 'longitude'] 값을 소수점 3째 자리까지 표준화합니다...\n",
      "\n",
      "처리 완료! 91 개의 공통 행을 찾았습니다.\n",
      "'common_events_matched.csv' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:12:23.650212Z",
     "start_time": "2025-11-03T13:12:10.870941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import time\n",
    "\n",
    "# --- 1단계: 로컬 파일 읽기 ---\n",
    "file_path = 'query (7).csv'\n",
    "print(f\"Loading local file: {file_path}\")\n",
    "\n",
    "try:\n",
    "    earthquake_list_df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded {len(earthquake_list_df)} events from file.\")\n",
    "\n",
    "\n",
    "    # --- 2단계: P/S파 데이터 수집 (수정됨) ---\n",
    "\n",
    "    client = Client(\"IRIS\")\n",
    "    phase_data_list = []\n",
    "\n",
    "    # (!! 테스트를 위해 처음 10개만 실행합니다. 전체는 .head(10)을 지우세요 !!)\n",
    "    print(f\"\\nPart 2: Fetching P/S phase data for the first 10 events (Wide Search)...\")\n",
    "\n",
    "    for index, row in earthquake_list_df.head(10).iterrows():\n",
    "        event_time = UTCDateTime(row['time'])\n",
    "\n",
    "        print(f\"\\nProcessing Event at {event_time.date} {event_time.time} (Mag {row['mag']})\")\n",
    "\n",
    "        try:\n",
    "            # === 여기가 수정된 부분입니다 ===\n",
    "            # 시간, 위치, 규모의 검색 범위를 넓힙니다.\n",
    "\n",
    "            catalog = client.get_events(\n",
    "                starttime = event_time - 30,  # 지진 발생 30초 전\n",
    "                endtime = event_time + 30,    # 지진 발생 30초 후\n",
    "                latitude = row['latitude'],\n",
    "                longitude = row['longitude'],\n",
    "                maxradius = 1.0,              # ★중요: 반경 1도 (약 111km) 이내에서 검색\n",
    "                minmagnitude = row['mag'] - 0.3, # 규모 오차범위 넓힘\n",
    "                maxmagnitude = row['mag'] + 0.3, # 규모 오차범위 넓힘\n",
    "                includearrivals = True       # P/S파 도착 정보 포함\n",
    "            )\n",
    "\n",
    "            if not catalog:\n",
    "                print(f\"  > No event found on IRIS matching this wide search.\")\n",
    "                continue\n",
    "\n",
    "            # 검색된 이벤트 (가장 첫 번째 것)\n",
    "            event = catalog[0]\n",
    "\n",
    "            # 이벤트의 'preferred' (가장 공신력 있는) 진원 정보를 가져옵니다.\n",
    "            origin = event.preferred_origin() or event.origins[0]\n",
    "\n",
    "            p_arrivals_count = 0\n",
    "            s_arrivals_count = 0\n",
    "\n",
    "            # 진원 정보에 연결된 '도착(arrivals)' 목록을 확인합니다.\n",
    "            if origin.arrivals:\n",
    "                for arrival in origin.arrivals:\n",
    "                    phase = arrival.phase\n",
    "                    if phase.upper().startswith('P'):\n",
    "                        p_arrivals_count += 1\n",
    "                    elif phase.upper().startswith('S'):\n",
    "                        s_arrivals_count += 1\n",
    "\n",
    "            print(f\"  > Found {p_arrivals_count} P-wave arrivals, {s_arrivals_count} S-wave arrivals.\")\n",
    "\n",
    "            phase_data_list.append({\n",
    "                'id': row['id'], # id는 원래 USGS id를 그대로 저장\n",
    "                'time': event_time,\n",
    "                'latitude': row['latitude'],\n",
    "                'longitude': row['longitude'],\n",
    "                'mag': row['mag'],\n",
    "                'p_wave_arrival_count': p_arrivals_count,\n",
    "                's_wave_arrival_count': s_arrivals_count\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"404\" in str(e) or \"No data available\" in str(e):\n",
    "                print(f\"  > No phase data found for this event.\")\n",
    "            else:\n",
    "                print(f\"  > Could not retrieve phase data: {e}\")\n",
    "\n",
    "        time.sleep(1) # IRIS 서버는 USGS보다 더 엄격하므로 1초 대기\n",
    "\n",
    "    # 7. 최종 결과를 DataFrame으로 변환\n",
    "    phase_df = pd.DataFrame(phase_data_list)\n",
    "\n",
    "    print(\"\\n--- Final P/S Phase Data Summary (Test Run) ---\")\n",
    "    print(phase_df)\n",
    "\n",
    "    # 8. 파일로 저장\n",
    "    output_filename = \"phase_data_from_query(7).csv\"\n",
    "    phase_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nSuccessfully saved data to {output_filename}\")\n",
    "\n",
    "# 1단계 파일 읽기 실패 시\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found. Make sure it's in the same directory as your script.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ],
   "id": "6f3af8fc7119dbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local file: query (7).csv\n",
      "Successfully loaded 6768 events from file.\n",
      "\n",
      "Part 2: Fetching P/S phase data for the first 10 events (Wide Search)...\n",
      "\n",
      "Processing Event at 2012-12-31 20:02:04.160000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-31 01:15:18.770000 (Mag 5.1)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 20:49:27.830000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 12:21:21.870000 (Mag 5.3)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 11:33:43.030000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 07:56:52.070000 (Mag 5.0)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 06:44:41.480000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-29 23:05:24.200000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-29 17:50:52.840000 (Mag 5.5)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-29 14:59:38.910000 (Mag 5.4)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "--- Final P/S Phase Data Summary (Test Run) ---\n",
      "           id                         time  latitude  longitude  mag  \\\n",
      "0  usp000jxp7  2012-12-31T20:02:04.160000Z   -61.623    154.301  5.2   \n",
      "1  usp000jxn3  2012-12-31T01:15:18.770000Z    15.031    -92.037  5.1   \n",
      "2  usp000jxmz  2012-12-30T20:49:27.830000Z   -12.861    -70.938  5.2   \n",
      "3  usp000jxmd  2012-12-30T12:21:21.870000Z    14.446    -92.991  5.3   \n",
      "4  usp000jxmb  2012-12-30T11:33:43.030000Z     1.569    126.587  5.2   \n",
      "5  usp000jxkx  2012-12-30T07:56:52.070000Z    -6.960    105.283  5.0   \n",
      "6  usp000jxks  2012-12-30T06:44:41.480000Z   -60.906    -36.887  5.2   \n",
      "7  usp000jxkc  2012-12-29T23:05:24.200000Z    37.018    141.168  5.2   \n",
      "8  usp000jxjw  2012-12-29T17:50:52.840000Z    35.711     70.599  5.5   \n",
      "9  usp000jxjr  2012-12-29T14:59:38.910000Z    38.738    142.024  5.4   \n",
      "\n",
      "   p_wave_arrival_count  s_wave_arrival_count  \n",
      "0                     0                     0  \n",
      "1                     0                     0  \n",
      "2                     0                     0  \n",
      "3                     0                     0  \n",
      "4                     0                     0  \n",
      "5                     0                     0  \n",
      "6                     0                     0  \n",
      "7                     0                     0  \n",
      "8                     0                     0  \n",
      "9                     0                     0  \n",
      "\n",
      "Successfully saved data to phase_data_from_query(7).csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:13:04.495598Z",
     "start_time": "2025-11-03T13:12:51.528118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import time\n",
    "\n",
    "# --- 1단계: 로컬 파일 읽기 ---\n",
    "file_path = 'query (7).csv'\n",
    "print(f\"Loading local file: {file_path}\")\n",
    "\n",
    "try:\n",
    "    earthquake_list_df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded {len(earthquake_list_df)} events from file.\")\n",
    "\n",
    "\n",
    "    # --- 2단계: P/S파 데이터 수집 (수정됨) ---\n",
    "\n",
    "    # 2-1. IRIS 클라이언트로 접속\n",
    "    client = Client(\"IRIS\")\n",
    "\n",
    "    phase_data_list = []\n",
    "\n",
    "    # (!! 테스트를 위해 처음 10개만 실행합니다. 전체는 .head(10)을 지우세요 !!)\n",
    "    print(f\"\\nPart 2: Fetching P/S phase data for the first 10 events (Wide Search)...\")\n",
    "\n",
    "    for index, row in earthquake_list_df.head(10).iterrows():\n",
    "        event_time = UTCDateTime(row['time'])\n",
    "\n",
    "        print(f\"\\nProcessing Event at {event_time.date} {event_time.time} (Mag {row['mag']})\")\n",
    "\n",
    "        try:\n",
    "            # 2-2. 시간/위치/규모로 IRIS 서버에서 지진 검색\n",
    "            catalog = client.get_events(\n",
    "                starttime = event_time - 30,  # 지진 발생 30초 전\n",
    "                endtime = event_time + 30,    # 지진 발생 30초 후\n",
    "                latitude = row['latitude'],\n",
    "                longitude = row['longitude'],\n",
    "                maxradius = 1.0,              # 반경 1도 (약 111km) 이내\n",
    "                minmagnitude = row['mag'] - 0.3,\n",
    "                maxmagnitude = row['mag'] + 0.3,\n",
    "                includearrivals = True       # ★ P/S파 도착 정보 포함\n",
    "            )\n",
    "\n",
    "            if not catalog:\n",
    "                print(f\"  > No event found on IRIS matching this wide search.\")\n",
    "                continue\n",
    "\n",
    "            # 검색된 이벤트 (가장 첫 번째 것)\n",
    "            event = catalog[0]\n",
    "\n",
    "            p_arrivals_count = 0\n",
    "            s_arrivals_count = 0\n",
    "\n",
    "            # === 여기가 수정된 부분입니다 ===\n",
    "            # 2-3. 'preferred_origin' 1개만 보는 대신, '모든' origins를 확인합니다.\n",
    "            if event.origins:\n",
    "                for origin in event.origins: # 모든 진원 정보를 반복\n",
    "                    if origin.arrivals: # P/S파 도착 정보가 있다면\n",
    "                        for arrival in origin.arrivals:\n",
    "                            phase = arrival.phase\n",
    "                            if phase.upper().startswith('P'):\n",
    "                                p_arrivals_count += 1\n",
    "                            elif phase.upper().startswith('S'):\n",
    "                                s_arrivals_count += 1\n",
    "\n",
    "            print(f\"  > Found {p_arrivals_count} P-wave arrivals, {s_arrivals_count} S-wave arrivals.\")\n",
    "\n",
    "            phase_data_list.append({\n",
    "                'id': row['id'], # id는 원래 USGS id를 그대로 저장\n",
    "                'time': event_time,\n",
    "                'latitude': row['latitude'],\n",
    "                'longitude': row['longitude'],\n",
    "                'mag': row['mag'],\n",
    "                'p_wave_arrival_count': p_arrivals_count,\n",
    "                's_wave_arrival_count': s_arrivals_count\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"204\" in str(e) or \"No data available\" in str(e):\n",
    "                print(f\"  > No phase data found for this event (204 No Content).\")\n",
    "                phase_data_list.append({\n",
    "                    'id': row['id'], 'time': event_time, 'latitude': row['latitude'],\n",
    "                    'longitude': row['longitude'], 'mag': row['mag'],\n",
    "                    'p_wave_arrival_count': 0, 's_wave_arrival_count': 0\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  > Could not retrieve phase data: {e}\")\n",
    "\n",
    "        time.sleep(1) # IRIS 서버에 부담을 주지 않기 위해 1초 대기\n",
    "\n",
    "    # 7. 최종 결과를 DataFrame으로 변환\n",
    "    phase_df = pd.DataFrame(phase_data_list)\n",
    "\n",
    "    print(\"\\n--- Final P/S Phase Data Summary (Test Run) ---\")\n",
    "    print(phase_df)\n",
    "\n",
    "    # 8. 파일로 저장\n",
    "    output_filename = \"phase_data_from_query(7).csv\"\n",
    "    phase_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nSuccessfully saved data to {output_filename}\")\n",
    "\n",
    "# 1단계 파일 읽기 실패 시\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found. Make sure it's in the same directory as your script.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ],
   "id": "e59baddb1ae3edc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local file: query (7).csv\n",
      "Successfully loaded 6768 events from file.\n",
      "\n",
      "Part 2: Fetching P/S phase data for the first 10 events (Wide Search)...\n",
      "\n",
      "Processing Event at 2012-12-31 20:02:04.160000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-31 01:15:18.770000 (Mag 5.1)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 20:49:27.830000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 12:21:21.870000 (Mag 5.3)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 11:33:43.030000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 07:56:52.070000 (Mag 5.0)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-30 06:44:41.480000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-29 23:05:24.200000 (Mag 5.2)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-29 17:50:52.840000 (Mag 5.5)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "Processing Event at 2012-12-29 14:59:38.910000 (Mag 5.4)\n",
      "  > Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "--- Final P/S Phase Data Summary (Test Run) ---\n",
      "           id                         time  latitude  longitude  mag  \\\n",
      "0  usp000jxp7  2012-12-31T20:02:04.160000Z   -61.623    154.301  5.2   \n",
      "1  usp000jxn3  2012-12-31T01:15:18.770000Z    15.031    -92.037  5.1   \n",
      "2  usp000jxmz  2012-12-30T20:49:27.830000Z   -12.861    -70.938  5.2   \n",
      "3  usp000jxmd  2012-12-30T12:21:21.870000Z    14.446    -92.991  5.3   \n",
      "4  usp000jxmb  2012-12-30T11:33:43.030000Z     1.569    126.587  5.2   \n",
      "5  usp000jxkx  2012-12-30T07:56:52.070000Z    -6.960    105.283  5.0   \n",
      "6  usp000jxks  2012-12-30T06:44:41.480000Z   -60.906    -36.887  5.2   \n",
      "7  usp000jxkc  2012-12-29T23:05:24.200000Z    37.018    141.168  5.2   \n",
      "8  usp000jxjw  2012-12-29T17:50:52.840000Z    35.711     70.599  5.5   \n",
      "9  usp000jxjr  2012-12-29T14:59:38.910000Z    38.738    142.024  5.4   \n",
      "\n",
      "   p_wave_arrival_count  s_wave_arrival_count  \n",
      "0                     0                     0  \n",
      "1                     0                     0  \n",
      "2                     0                     0  \n",
      "3                     0                     0  \n",
      "4                     0                     0  \n",
      "5                     0                     0  \n",
      "6                     0                     0  \n",
      "7                     0                     0  \n",
      "8                     0                     0  \n",
      "9                     0                     0  \n",
      "\n",
      "Successfully saved data to phase_data_from_query(7).csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "base_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "all_earthquakes = []\n",
    "\n",
    "print(\"Part 1: Downloading Earthquake List (2001-2022)...\")\n",
    "\n",
    "# 2001년부터 2022년까지 1년 단위로 반복\n",
    "for year in range(2001, 2023):\n",
    "    print(f\"Fetching data for {year}...\")\n",
    "    params = {\n",
    "        'format': 'csv',\n",
    "        'starttime': f'{year}-01-01',\n",
    "        'endtime': f'{year}-12-31',\n",
    "        'minmagnitude': 5.0,  # P/S파 분석을 위해 최소 규모 5.0 이상으로 제한\n",
    "        'orderby': 'time',\n",
    "        'eventtype': 'earthquake'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            csv_data = io.StringIO(response.text)\n",
    "            df_year = pd.read_csv(csv_data)\n",
    "            all_earthquakes.append(df_year)\n",
    "            print(f\"  > Success: Found {len(df_year)} events in {year}.\")\n",
    "        else:\n",
    "            print(f\"  > Failed for {year}. Status: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  > Error during request for {year}: {e}\")\n",
    "\n",
    "# 모든 연도별 데이터를 하나로 합치기\n",
    "earthquake_list_df = pd.concat(all_earthquakes, ignore_index=True)\n",
    "print(f\"\\nTotal events found from 2001-2022 (Mag 5.0+): {len(earthquake_list_df)}\")\n",
    "print(earthquake_list_df.head())"
   ],
   "id": "3bdbaa3be2bde077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import time\n",
    "\n",
    "# 1. IRIS 클라이언트 접속\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "# 2. P파, S파 도착 시간을 저장할 리스트\n",
    "phase_data_list = []\n",
    "\n",
    "# 3. 1단계에서 만든 목록(earthquake_list_df)을 사용\n",
    "# (!! 테스트를 위해 처음 10개만 실행해 보세요 !!)\n",
    "# (전부 실행하려면 [:] 로 바꾸세요)\n",
    "for index, row in earthquake_list_df.head(10).iterrows():\n",
    "    event_id = row['id']\n",
    "    event_time = UTCDateTime(row['time'])\n",
    "\n",
    "    print(f\"\\nProcessing Event ID: {event_id} ({event_time.date})\")\n",
    "\n",
    "    try:\n",
    "        # 4. 'eventid'를 사용해 'phase data'가 포함된 이벤트 정보 요청\n",
    "        catalog = client.get_events(eventid=event_id, includeallorigins=True, includeallphases=True)\n",
    "\n",
    "        # 5. 첫 번째 이벤트의 정보 (보통 하나만 나옴)\n",
    "        event = catalog[0]\n",
    "\n",
    "        # 6. 모든 도착 정보(arrivals)에서 P파, S파 정보 추출\n",
    "        p_arrivals = 0\n",
    "        s_arrivals = 0\n",
    "\n",
    "        if event.picks:\n",
    "            for pick in event.picks:\n",
    "                phase_hint = pick.phase_hint\n",
    "                if phase_hint in ['P', 'p', 'Pn', 'Pg']:\n",
    "                    p_arrivals += 1\n",
    "                elif phase_hint in ['S', 's', 'Sn', 'Sg']:\n",
    "                    s_arrivals += 1\n",
    "\n",
    "        print(f\"  > Found {p_arrivals} P-wave arrivals, {s_arrivals} S-wave arrivals.\")\n",
    "\n",
    "        phase_data_list.append({\n",
    "            'id': event_id,\n",
    "            'time': event_time,\n",
    "            'latitude': row['latitude'],\n",
    "            'longitude': row['longitude'],\n",
    "            'mag': row['mag'],\n",
    "            'p_wave_arrival_count': p_arrivals, # P파가 관측된 관측소 수\n",
    "            's_wave_arrival_count': s_arrivals  # S파가 관측된 관측소 수\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  > Could not retrieve phase data for {event_id}: {e}\")\n",
    "\n",
    "    # (API 서버에 부담을 주지 않기 위해 잠시 대기)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# 7. 최종 결과를 DataFrame으로 변환\n",
    "phase_df = pd.DataFrame(phase_data_list)\n",
    "\n",
    "print(\"\\n--- Final P/S Phase Data Summary ---\")\n",
    "print(phase_df)\n",
    "\n",
    "# 파일로 저장\n",
    "phase_df.to_csv(\"earthquake_phase_data_2001-2022.csv\", index=False)"
   ],
   "id": "59c36bf1812b632a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:34:29.728907Z",
     "start_time": "2025-11-03T13:34:27.996668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "import time\n",
    "\n",
    "# 1. IRIS 클라이언트로 접속\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "def get_detailed_phases(event_id, event_mag):\n",
    "    \"\"\"지진 ID를 받아서 상세 P/S파 개수를 세는 함수\"\"\"\n",
    "\n",
    "    print(f\"\\nProcessing Event ID: {event_id} (Mag {event_mag})\")\n",
    "    p_arrivals_count = 0\n",
    "    s_arrivals_count = 0\n",
    "\n",
    "    try:\n",
    "        # 'eventid' 파라미터를 사용하여 상세 정보 요청\n",
    "        detailed_catalog = client.get_events(\n",
    "            eventid=event_id,\n",
    "            includearrivals=True\n",
    "        )\n",
    "\n",
    "        if not detailed_catalog:\n",
    "            print(\"  > Could not fetch detailed catalog for this event.\")\n",
    "            return p_arrivals_count, s_arrivals_count\n",
    "\n",
    "        # \"상세 정보\"에서 'picks'가 아니라 'origins' 안의 'arrivals'를 확인\n",
    "        detailed_event = detailed_catalog[0]\n",
    "\n",
    "        # === THIS IS THE NEW LOGIC ===\n",
    "        if detailed_event.origins:\n",
    "            # 모든 origin을 다 확인\n",
    "            for origin in detailed_event.origins:\n",
    "                if origin.arrivals:\n",
    "                    for arrival in origin.arrivals:\n",
    "                        phase = arrival.phase\n",
    "                        if phase and phase.upper().startswith('P'):\n",
    "                            p_arrivals_count += 1\n",
    "                        elif phase and phase.upper().startswith('S'):\n",
    "                            s_arrivals_count += 1\n",
    "        # =============================\n",
    "\n",
    "        print(f\"  > SUCCESS: Found {p_arrivals_count} P-wave arrivals, {s_arrivals_count} S-wave arrivals.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"204\" in str(e) or \"No data available\" in str(e):\n",
    "            print(f\"  > No detailed phase data found for this event (204 No Content).\")\n",
    "        else:\n",
    "            print(f\"  > Error fetching details: {e}\")\n",
    "\n",
    "    return p_arrivals_count, s_arrivals_count\n",
    "\n",
    "# --- 테스트 1: 님이 주신 2022년 데이터 (결과가 0일 것으로 예상) ---\n",
    "print(\"--- TEST 1: Your 2022 Event ---\")\n",
    "test_1_id = \"11640145\" # 2022년 목록 첫 번째 ID\n",
    "test_1_mag = 5.1\n",
    "p_count_1, s_count_1 = get_detailed_phases(test_1_id, test_1_mag)\n",
    "\n",
    "time.sleep(1) # 1초 대기\n",
    "\n",
    "# --- 테스트 2: 2011년 동일본 대지진 (결과가 0이 아니어야 함) ---\n",
    "print(\"\\n--- TEST 2: 2011 Japan Earthquake (Tohoku) ---\")\n",
    "# 2011년 동일본 대지진의 IRIS ID (GCMT 카탈로그 기준)\n",
    "test_2_id = \"1418931\"\n",
    "test_2_mag = 9.1\n",
    "p_count_2, s_count_2 = get_detailed_phases(test_2_id, test_2_mag)\n",
    "\n",
    "\n",
    "# --- 최종 결과 요약 ---\n",
    "print(\"\\n--- TEST RESULTS ---\")\n",
    "print(f\"Test 1 (Your 2022 Event, ID: {test_1_id}): P={p_count_1}, S={s_count_1}\")\n",
    "print(f\"Test 2 (2011 Japan Event, ID: {test_2_id}): P={p_count_2}, S={s_count_2}\")\n",
    "\n",
    "if p_count_1 == 0 and p_count_2 > 0:\n",
    "    print(\"\\n[결론]\")\n",
    "    print(\"코드는 정상적으로 작동합니다.\")\n",
    "    print(\"하지만 2022년의 규모 5.0 지진들은 IRIS 데이터베이스에 P/S파 'picks' 정보가 등록되어 있지 않습니다 (0개가 맞습니다).\")\n",
    "else:\n",
    "    print(\"\\n[결론]\")\n",
    "    print(\"테스트 2(2011년 지진)에서도 0이 나온다면, 코드나 네트워크에 다른 문제가 있는 것입니다.\")"
   ],
   "id": "d547a1b25dd46af9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST 1: Your 2022 Event ---\n",
      "\n",
      "Processing Event ID: 11640145 (Mag 5.1)\n",
      "  > SUCCESS: Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "--- TEST 2: 2011 Japan Earthquake (Tohoku) ---\n",
      "\n",
      "Processing Event ID: 1418931 (Mag 9.1)\n",
      "  > SUCCESS: Found 0 P-wave arrivals, 0 S-wave arrivals.\n",
      "\n",
      "--- TEST RESULTS ---\n",
      "Test 1 (Your 2022 Event, ID: 11640145): P=0, S=0\n",
      "Test 2 (2011 Japan Event, ID: 1418931): P=0, S=0\n",
      "\n",
      "[결론]\n",
      "테스트 2(2011년 지진)에서도 0이 나온다면, 코드나 네트워크에 다른 문제가 있는 것입니다.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T14:05:27.655700Z",
     "start_time": "2025-11-03T14:05:26.643844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import obspy\n",
    "from obspy.clients.fdsn import Client\n",
    "import inspect # 경로 추적을 위한 모듈\n",
    "import time\n",
    "\n",
    "print(f\"--- 현재 obspy 버전: {obspy.__version__} ---\")\n",
    "\n",
    "# ---!!! 결정적인 진단 코드 !!!---\n",
    "# Client 클래스가 로드된 실제 파일 경로를 출력합니다.\n",
    "# 이 경로가 1.4.2가 설치된 경로(예: .../site-packages/obspy/clients/fdsn/client.py)여야 합니다.\n",
    "try:\n",
    "    print(f\"--- Client 클래스 파일 위치: {inspect.getfile(Client)} ---\")\n",
    "except Exception as e:\n",
    "    print(f\"--- Client 클래스 파일 위치 추적 실패: {e} ---\")\n",
    "\n",
    "# ---!!! get_picks 함수가 존재하는지 다시 확인 ---\n",
    "if hasattr(Client(\"IRIS\"), 'get_picks'):\n",
    "    print(\"--- 진단: get_picks() 함수가 정상적으로 존재합니다. ---\")\n",
    "else:\n",
    "    print(\"--- !!!심각한 오류: get_picks() 함수가 여전히 없습니다. 설치가 완전히 꼬였습니다.!!! ---\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "# 이전에 사용했던 'get_picks' 함수 (이제 작동해야 함)\n",
    "def get_picks_for_event(event_id, event_mag):\n",
    "    print(f\"\\nProcessing Event ID: {event_id} (Mag {event_mag}) using get_picks()\")\n",
    "    p_pick_count = 0\n",
    "    s_pick_count = 0\n",
    "\n",
    "    client = Client(\"IRIS\") # 객체 새로 생성\n",
    "\n",
    "    try:\n",
    "        picks_catalog = client.get_picks(\n",
    "            eventid=event_id\n",
    "        )\n",
    "\n",
    "        if not picks_catalog:\n",
    "            print(f\"  > No associated 'Pick' data found for Event ID {event_id}.\")\n",
    "            return p_pick_count, s_pick_count\n",
    "\n",
    "        print(f\"  > SUCCESS: Found {len(picks_catalog)} total 'Picks' associated with this event.\")\n",
    "\n",
    "        for pick in picks_catalog:\n",
    "            phase = pick.phase_hint\n",
    "            if phase:\n",
    "                phase_upper = phase.upper()\n",
    "                if phase_upper.startswith('P'):\n",
    "                    p_pick_count += 1\n",
    "                elif phase_upper.startswith('S'):\n",
    "                    s_pick_count += 1\n",
    "\n",
    "        print(f\"  > P-Picks: {p_pick_count}\")\n",
    "        print(f\"  > S-Picks: {s_pick_count}\")\n",
    "\n",
    "    except AttributeError as ae:\n",
    "        print(f\"  > !!! AttributeError 발생: {ae}\")\n",
    "        print(\"  > (원인: 환경이 여전히 꼬여있습니다. 위 '강제 재설치'를 다시 시도하세요.)\")\n",
    "    except Exception as e:\n",
    "        if \"204\" in str(e) or \"No data available\" in str(e):\n",
    "            print(f\"  > No associated 'Pick' data found for Event ID {event_id} (204 No Content).\")\n",
    "        else:\n",
    "            print(f\"  > Error fetching picks: {e}\")\n",
    "\n",
    "    return p_pick_count, s_pick_count\n",
    "\n",
    "# --- 테스트 1: 2022년 데이터 ---\n",
    "print(\"\\n--- TEST 1: Your 2022 Event (using get_picks) ---\")\n",
    "test_1_id = \"11640145\"\n",
    "test_1_mag = 5.1\n",
    "p_count_1, s_count_1 = get_picks_for_event(test_1_id, test_1_mag)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# --- 테스트 2: 2011년 동일본 대지진 ---\n",
    "print(\"\\n--- TEST 2: 2011 Japan Earthquake (using get_picks) ---\")\n",
    "test_2_id = \"1418931\"\n",
    "test_2_mag = 9.1\n",
    "p_count_2, s_count_2 = get_picks_for_event(test_2_id, test_2_mag)\n",
    "\n",
    "# --- 최종 결과 요약 ---\n",
    "print(\"\\n--- TEST RESULTS (using get_picks) ---\")\n",
    "print(f\"Test 1 (Your 2022 Event, ID: {test_1_id}): P={p_count_1}, S={s_count_1}\")\n",
    "print(f\"Test 2 (2011 Japan Event, ID: {test_2_id}): P={p_count_2}, S={s_count_2}\")"
   ],
   "id": "9f6022c249be5b4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 현재 obspy 버전: 1.4.2 ---\n",
      "--- Client 클래스 파일 위치: C:\\Users\\admin\\Desktop\\3-team-project\\team_project_python\\.venv\\Lib\\site-packages\\obspy\\clients\\fdsn\\client.py ---\n",
      "--- !!!심각한 오류: get_picks() 함수가 여전히 없습니다. 설치가 완전히 꼬였습니다.!!! ---\n",
      "\n",
      "--- TEST 1: Your 2022 Event (using get_picks) ---\n",
      "\n",
      "Processing Event ID: 11640145 (Mag 5.1) using get_picks()\n",
      "  > !!! AttributeError 발생: 'Client' object has no attribute 'get_picks'\n",
      "  > (원인: 환경이 여전히 꼬여있습니다. 위 '강제 재설치'를 다시 시도하세요.)\n",
      "\n",
      "--- TEST 2: 2011 Japan Earthquake (using get_picks) ---\n",
      "\n",
      "Processing Event ID: 1418931 (Mag 9.1) using get_picks()\n",
      "  > !!! AttributeError 발생: 'Client' object has no attribute 'get_picks'\n",
      "  > (원인: 환경이 여전히 꼬여있습니다. 위 '강제 재설치'를 다시 시도하세요.)\n",
      "\n",
      "--- TEST RESULTS (using get_picks) ---\n",
      "Test 1 (Your 2022 Event, ID: 11640145): P=0, S=0\n",
      "Test 2 (2011 Japan Event, ID: 1418931): P=0, S=0\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
