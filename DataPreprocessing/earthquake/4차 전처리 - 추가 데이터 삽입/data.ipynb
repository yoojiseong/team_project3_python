{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 파일 이름 정의\n",
    "input_file = '../3차 전처리 - 단층 이동 데이터/TEST_train_data_final_with_fault_counts_past_year_full_month.csv'\n",
    "output_file = 'final_model_data.csv' # (최종 훈련용 데이터)\n",
    "\n",
    "# 2. (핵심) 남기고자 하는 11개의 최종 컬럼 목록\n",
    "columns_to_keep = [\n",
    "    'magnitude',\n",
    "    'depth',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'tsunami',\n",
    "    'is_ocean',\n",
    "    'is_steep_slope',\n",
    "    'horizontal_count_1y_full',\n",
    "    'vertical_count_1y_full'\n",
    "]\n",
    "\n",
    "try:\n",
    "    # 3. 데이터 불러오기\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"'{input_file}' 파일 (총 {len(df)}행)을 불러왔습니다.\")\n",
    "\n",
    "    # 4. (전처리) 'is_ocean', 'is_steep_slope'이 0/1이 아닐 경우를 대비\n",
    "    if 'is_ocean' in df.columns:\n",
    "        df['is_ocean'] = df['is_ocean'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "    if 'is_steep_slope' in df.columns:\n",
    "        df['is_steep_slope'] = df['is_steep_slope'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "\n",
    "    # 5. (핵심) 11개 컬럼만 선택 (필터링)\n",
    "    # df에 있지만 columns_to_keep에 없는 'nst', 'dmin', 'gap' 등은 모두 제거됨\n",
    "\n",
    "    # (안전장치) 요청한 컬럼 중 실제 파일에 있는 것만 필터링\n",
    "    existing_columns_to_keep = [col for col in columns_to_keep if col in df.columns]\n",
    "\n",
    "    df_filtered = df[existing_columns_to_keep]\n",
    "\n",
    "    print(f\"\\n{len(existing_columns_to_keep)}개의 요청된 컬럼만 남겼습니다:\")\n",
    "    print(existing_columns_to_keep)\n",
    "\n",
    "    # 6. 새 파일로 저장\n",
    "    df_filtered.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"\\n--- 처리 완료! ---\")\n",
    "    print(f\"모든 필터링이 완료된 최종 데이터가 '{output_file}'에 저장되었습니다.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}' 파일을 찾을 수 없습니다.\")\n",
    "except KeyError as e:\n",
    "    print(f\"오류: 요청한 컬럼 중 일부가 원본 파일에 없습니다. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"알 수 없는 오류 발생: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T13:36:49.074800Z",
     "start_time": "2025-11-03T13:30:06.804786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta, UTC\n",
    "from calendar import monthrange\n",
    "from geopy.distance import geodesic\n",
    "from geopy.point import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. ★★★ 설정 (중요) ★★★ ---\n",
    "# ⚠️ 여기에 Google API 키를 입력하세요\n",
    "GOOGLE_API_KEY = \"AIzaSyCQt1-_LLhTfX_0l6JAZU1WwlZ1ldkTVTw\"\n",
    "\n",
    "# --- 2. 모델링 파라미터 (V18과 동일) ---\n",
    "SLOPE_RADIUS_KM = 60\n",
    "SLOPE_THRESHOLD_METERS = 2000\n",
    "USGS_RADIUS_KM = 200\n",
    "USGS_MIN_MAGNITUDE = 6.0 # 과거 1년 검색용\n",
    "\n",
    "# --- 3. 헬퍼 함수 (V21과 동일) ---\n",
    "# (get_elevation_features, get_usgs_fault_counts 등 모든 헬퍼 함수들)\n",
    "\n",
    "def get_surrounding_points(lat, lon, radius_km):\n",
    "    center_point = Point(lat, lon)\n",
    "    points = {'center': (lat, lon)}\n",
    "    bearings = [0, 90, 180, 270]; names = ['north', 'east', 'south', 'west']\n",
    "    for name, bearing in zip(names, bearings):\n",
    "        destination = geodesic(kilometers=radius_km).destination(center_point, bearing)\n",
    "        points[name] = (destination.latitude, destination.longitude)\n",
    "    return points\n",
    "\n",
    "def get_elevation_features(lat, lon):\n",
    "    is_ocean, is_steep_slope = 0, 0\n",
    "    if GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
    "        print(\"  [오류] Elevation API 키가 설정되지 않았습니다. (0, 0) 반환.\")\n",
    "        return pd.Series([0, 0])\n",
    "    try:\n",
    "        points_to_check = get_surrounding_points(lat, lon, SLOPE_RADIUS_KM)\n",
    "        locations_list = [\n",
    "            points_to_check['center'], points_to_check['north'],\n",
    "            points_to_check['east'], points_to_check['south'], points_to_check['west']\n",
    "        ]\n",
    "        locations_str = \"|\".join([f\"{lt},{ln}\" for lt, ln in locations_list])\n",
    "        url = \"https://maps.googleapis.com/maps/api/elevation/json\"\n",
    "        params = {'locations': locations_str, 'key': GOOGLE_API_KEY}\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data['status'] == 'OK':\n",
    "            results = data['results']\n",
    "            if not results: return pd.Series([0, 0])\n",
    "            center_elevation = results[0]['elevation']\n",
    "            if center_elevation < 0: is_ocean = 1\n",
    "            surrounding_elevations = [res['elevation'] for res in results[1:]]\n",
    "            for elev in surrounding_elevations:\n",
    "                if abs(center_elevation - elev) > SLOPE_THRESHOLD_METERS:\n",
    "                    is_steep_slope = 1\n",
    "                    break\n",
    "        return pd.Series([is_ocean, is_steep_slope])\n",
    "    except Exception as e:\n",
    "        print(f\"  [오류] Elevation API (Row) 실패: {e}. (0, 0) 반환.\")\n",
    "        return pd.Series([0, 0])\n",
    "\n",
    "def get_usgs_fault_counts(row):\n",
    "    horizontal_count, vertical_count = 0, 0\n",
    "    try:\n",
    "        # (수정) 이 지진이 발생한 시점(Year, Month)을 기준으로 '과거 1년'을 검색\n",
    "        year = int(row['Year'])\n",
    "        month = int(row['Month'])\n",
    "\n",
    "        endtime = f\"{year}-{month:02d}-01\"\n",
    "        starttime = f\"{year - 1}-{month:02d}-01\"\n",
    "\n",
    "        lat = row['latitude']\n",
    "        lon = row['longitude']\n",
    "\n",
    "        search_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "        params = {\n",
    "            'format': 'geojson', 'starttime': starttime, 'endtime': endtime,\n",
    "            'latitude': lat, 'longitude': lon, 'maxradiuskm': USGS_RADIUS_KM,\n",
    "            'minmagnitude': USGS_MIN_MAGNITUDE\n",
    "        }\n",
    "        response = requests.get(search_url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        found_quakes = data.get('features', [])\n",
    "        if not found_quakes: return pd.Series([0, 0])\n",
    "\n",
    "        for quake in found_quakes:\n",
    "            detail_url = quake['properties'].get('detail')\n",
    "            if not detail_url: continue\n",
    "            time.sleep(0.1)\n",
    "            detail_response = requests.get(detail_url, timeout=10)\n",
    "            detail_response.raise_for_status()\n",
    "            detail_data = detail_response.json()\n",
    "            products = detail_data['properties'].get('products', {})\n",
    "            rake_value = None\n",
    "            all_products = products.get('moment-tensor', []) + products.get('focal-mechanism', [])\n",
    "            if not all_products: continue\n",
    "            best_product = None\n",
    "            for p in all_products:\n",
    "                if 'gcmt' in p.get('id','').lower(): best_product = p; break\n",
    "            if best_product is None:\n",
    "                for p in all_products:\n",
    "                     if 'us' in p.get('id','').lower() or p.get('code','').lower() == 'us': best_product = p; break\n",
    "            if best_product is None: best_product = all_products[0]\n",
    "            if best_product:\n",
    "                props = best_product.get('properties', {})\n",
    "                rake_str = props.get('nodal-plane-1-rake')\n",
    "                if rake_str is None: rake_str = props.get('rake')\n",
    "                if rake_str is not None: rake_value = float(rake_str)\n",
    "            if rake_value is not None:\n",
    "                if (45 <= rake_value <= 135) or (-135 <= rake_value <= -45):\n",
    "                    vertical_count += 1\n",
    "                else:\n",
    "                    horizontal_count += 1\n",
    "        return pd.Series([horizontal_count, vertical_count])\n",
    "    except Exception as e:\n",
    "        print(f\"  [오류] USGS API (Row) 실패: {e}. (0, 0) 반환.\")\n",
    "        return pd.Series([0, 0])\n",
    "\n",
    "# --- 4. 메인 코드: 20개 \"왕\"급 데이터 처리 ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if GOOGLE_API_KEY == \"AIzaSyCQt1-_LLhTfX_0l6JAZU1WwlZ1ldkTVTw\":\n",
    "        print(\"=\"*50)\n",
    "        print(\"⚠️ 경고: 12번째 줄의 'GOOGLE_API_KEY'를 입력해야 스크립트가 작동합니다.\")\n",
    "        print(\"=\"*50)\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv('common_events_matched.csv')\n",
    "        print(f\"'common_events_matched.csv' 파일 (총 {len(df)}행) 처리 시작...\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"오류: 1단계에서 'common_events_matched.csv' 파일을 먼저 생성해야 합니다.\")\n",
    "        exit()\n",
    "\n",
    "    tqdm.pandas(desc=\"V22 API 처리 중\")\n",
    "\n",
    "    # (수정) 2개의 API 함수를 20개 행에 대해 순차적으로 실행\n",
    "\n",
    "    print(\"-> 1/2: Elevation API (is_ocean, is_steep_slope) 수집 중...\")\n",
    "    df_elevation = df.progress_apply(\n",
    "        lambda row: get_elevation_features(row['latitude'], row['longitude']),\n",
    "        axis=1\n",
    "    )\n",
    "    df_elevation.columns = ['is_ocean', 'is_steep_slope']\n",
    "\n",
    "    print(\"\\n-> 2/2: USGS API (Fault Counts) 수집 중...\")\n",
    "    df_usgs = df.progress_apply(\n",
    "        get_usgs_fault_counts,\n",
    "        axis=1\n",
    "    )\n",
    "    df_usgs.columns = ['horizontal_count_1y_full', 'vertical_count_1y_full']\n",
    "\n",
    "    # 원본(df)과 2개의 API 결과(df_elevation, df_usgs)를 하나로 합침\n",
    "    df_processed = pd.concat([df, df_elevation, df_usgs], axis=1)\n",
    "\n",
    "    # 4. 새 파일로 저장\n",
    "    output_filename = 'common_events_matched_proceed.csv'\n",
    "    df_processed.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\n--- 처리 완료! ---\")\n",
    "    print(f\"모든 특성이 추가된 20개의 데이터가 '{output_filename}'에 저장되었습니다.\")\n",
    "    print(df_processed.head())"
   ],
   "id": "2064d40479160ec3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "⚠️ 경고: 12번째 줄의 'GOOGLE_API_KEY'를 입력해야 스크립트가 작동합니다.\n",
      "==================================================\n",
      "'common_events_matched.csv' 파일 (총 91행) 처리 시작...\n",
      "-> 1/2: Elevation API (is_ocean, is_steep_slope) 수집 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V22 API 처리 중: 100%|██████████| 91/91 [00:55<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> 2/2: USGS API (Fault Counts) 수집 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V22 API 처리 중: 100%|██████████| 91/91 [05:46<00:00,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 처리 완료! ---\n",
      "모든 특성이 추가된 20개의 데이터가 'common_events_matched_proceed.csv'에 저장되었습니다.\n",
      "                       time  latitude  longitude  depth    nst   gap  mag  \\\n",
      "0  2012-09-05T14:42:07.800Z    10.085    -85.315   35.0  703.0  34.3  7.6   \n",
      "1  2012-08-31T12:47:33.380Z    10.811    126.638   28.0  682.0  10.2  7.6   \n",
      "2  2012-08-27T04:37:19.430Z    12.139    -88.590   28.0  417.0  37.0  7.3   \n",
      "3  2012-04-11T10:43:10.850Z     0.802     92.463   25.1  341.0  14.9  8.2   \n",
      "4  2012-04-11T08:38:36.720Z     2.327     93.063   20.0  499.0  16.6  8.6   \n",
      "\n",
      "   Year  Month  tsunami  is_ocean  is_steep_slope  horizontal_count_1y_full  \\\n",
      "0  2012      9        1         0               0                         0   \n",
      "1  2012      8        1         1               1                         0   \n",
      "2  2012      8        1         1               1                         0   \n",
      "3  2012      4        1         1               0                         1   \n",
      "4  2012      4        1         1               0                         1   \n",
      "\n",
      "   vertical_count_1y_full  \n",
      "0                       1  \n",
      "1                       0  \n",
      "2                       0  \n",
      "3                       0  \n",
      "4                       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:30:53.309426Z",
     "start_time": "2025-11-01T15:30:53.299155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. 파일 이름 정의 ---\n",
    "file_original_782 = '../3차 전처리 - 단층 이동 데이터/TEST_train_data_final_with_fault_counts_past_year_full_month.csv'\n",
    "file_new_20 = 'new_kings_processed.csv'\n",
    "file_output_802 = 'final_802_data_for_training.csv'\n",
    "\n",
    "# --- 2. 모델 훈련에 필요한 7개의 핵심 열 정의 ---\n",
    "# (이 순서가 모델이 학습한 순서입니다)\n",
    "MODEL_COLUMNS = [\n",
    "    'magnitude',\n",
    "    'depth',\n",
    "    'is_ocean',\n",
    "    'is_steep_slope',\n",
    "    'horizontal_count_1y_full',\n",
    "    'vertical_count_1y_full',\n",
    "    'tsunami' # 타겟 변수\n",
    "]\n",
    "\n",
    "# --- 3. 데이터 로드 및 필터링 ---\n",
    "try:\n",
    "    df_782 = pd.read_csv(file_original_782)\n",
    "    df_20 = pd.read_csv(file_new_20)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"오류: 파일이 없습니다. {e}\")\n",
    "    print(\"1단계('new_kings.csv')와 2단계('process_new_kings.py')를 먼저 실행하세요.\")\n",
    "    exit()\n",
    "\n",
    "# (전처리) 'is_ocean', 'is_steep_slope'이 0/1이 아닐 경우를 대비\n",
    "df_782['is_ocean'] = df_782['is_ocean'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "df_782['is_steep_slope'] = df_782['is_steep_slope'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "# (df_20은 V22에서 이미 0/1로 생성됨)\n",
    "\n",
    "# 7개의 핵심 열만 남기고 나머지(nst, dmin, Year 등)는 모두 버립니다.\n",
    "df_782_clean = df_782[MODEL_COLUMNS]\n",
    "df_20_clean = df_20[MODEL_COLUMNS]\n",
    "\n",
    "# --- 4. 데이터 병합 (Concat) ---\n",
    "df_final_802 = pd.concat([df_782_clean, df_20_clean], ignore_index=True)\n",
    "\n",
    "# --- 5. 최종 저장 ---\n",
    "df_final_802.to_csv(file_output_802, index=False)\n",
    "\n",
    "print(\"--- 병합 완료! ---\")\n",
    "print(f\"원본 데이터 782개 + '왕'급 데이터 20개 = 총 {len(df_final_802)}개\")\n",
    "print(f\"모델 훈련을 위한 최종 파일: '{file_output_802}'\")"
   ],
   "id": "735c836c63057287",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 병합 완료! ---\n",
      "원본 데이터 782개 + '왕'급 데이터 20개 = 총 802개\n",
      "모델 훈련을 위한 최종 파일: 'final_802_data_for_training.csv'\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
