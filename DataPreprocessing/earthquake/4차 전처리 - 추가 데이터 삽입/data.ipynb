{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 파일 이름 정의\n",
    "input_file = '../3차 전처리 - 단층 이동 데이터/TEST_train_data_final_with_fault_counts_past_year_full_month.csv'\n",
    "output_file = 'final_model_data.csv' # (최종 훈련용 데이터)\n",
    "\n",
    "# 2. (핵심) 남기고자 하는 11개의 최종 컬럼 목록\n",
    "columns_to_keep = [\n",
    "    'magnitude',\n",
    "    'depth',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'tsunami',\n",
    "    'is_ocean',\n",
    "    'is_steep_slope',\n",
    "    'horizontal_count_1y_full',\n",
    "    'vertical_count_1y_full'\n",
    "]\n",
    "\n",
    "try:\n",
    "    # 3. 데이터 불러오기\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"'{input_file}' 파일 (총 {len(df)}행)을 불러왔습니다.\")\n",
    "\n",
    "    # 4. (전처리) 'is_ocean', 'is_steep_slope'이 0/1이 아닐 경우를 대비\n",
    "    if 'is_ocean' in df.columns:\n",
    "        df['is_ocean'] = df['is_ocean'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "    if 'is_steep_slope' in df.columns:\n",
    "        df['is_steep_slope'] = df['is_steep_slope'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "\n",
    "    # 5. (핵심) 11개 컬럼만 선택 (필터링)\n",
    "    # df에 있지만 columns_to_keep에 없는 'nst', 'dmin', 'gap' 등은 모두 제거됨\n",
    "\n",
    "    # (안전장치) 요청한 컬럼 중 실제 파일에 있는 것만 필터링\n",
    "    existing_columns_to_keep = [col for col in columns_to_keep if col in df.columns]\n",
    "\n",
    "    df_filtered = df[existing_columns_to_keep]\n",
    "\n",
    "    print(f\"\\n{len(existing_columns_to_keep)}개의 요청된 컬럼만 남겼습니다:\")\n",
    "    print(existing_columns_to_keep)\n",
    "\n",
    "    # 6. 새 파일로 저장\n",
    "    df_filtered.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"\\n--- 처리 완료! ---\")\n",
    "    print(f\"모든 필터링이 완료된 최종 데이터가 '{output_file}'에 저장되었습니다.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}' 파일을 찾을 수 없습니다.\")\n",
    "except KeyError as e:\n",
    "    print(f\"오류: 요청한 컬럼 중 일부가 원본 파일에 없습니다. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"알 수 없는 오류 발생: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:30:19.392882Z",
     "start_time": "2025-11-01T15:29:21.169090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta, UTC\n",
    "from calendar import monthrange\n",
    "from geopy.distance import geodesic\n",
    "from geopy.point import Point\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. ★★★ 설정 (중요) ★★★ ---\n",
    "# ⚠️ 여기에 Google API 키를 입력하세요\n",
    "GOOGLE_API_KEY = \"AIzaSyCQt1-_LLhTfX_0l6JAZU1WwlZ1ldkTVTw\"\n",
    "\n",
    "# --- 2. 모델링 파라미터 (V18과 동일) ---\n",
    "SLOPE_RADIUS_KM = 60\n",
    "SLOPE_THRESHOLD_METERS = 2000\n",
    "USGS_RADIUS_KM = 200\n",
    "USGS_MIN_MAGNITUDE = 6.0 # 과거 1년 검색용\n",
    "\n",
    "# --- 3. 헬퍼 함수 (V21과 동일) ---\n",
    "# (get_elevation_features, get_usgs_fault_counts 등 모든 헬퍼 함수들)\n",
    "\n",
    "def get_surrounding_points(lat, lon, radius_km):\n",
    "    center_point = Point(lat, lon)\n",
    "    points = {'center': (lat, lon)}\n",
    "    bearings = [0, 90, 180, 270]; names = ['north', 'east', 'south', 'west']\n",
    "    for name, bearing in zip(names, bearings):\n",
    "        destination = geodesic(kilometers=radius_km).destination(center_point, bearing)\n",
    "        points[name] = (destination.latitude, destination.longitude)\n",
    "    return points\n",
    "\n",
    "def get_elevation_features(lat, lon):\n",
    "    is_ocean, is_steep_slope = 0, 0\n",
    "    if GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
    "        print(\"  [오류] Elevation API 키가 설정되지 않았습니다. (0, 0) 반환.\")\n",
    "        return pd.Series([0, 0])\n",
    "    try:\n",
    "        points_to_check = get_surrounding_points(lat, lon, SLOPE_RADIUS_KM)\n",
    "        locations_list = [\n",
    "            points_to_check['center'], points_to_check['north'],\n",
    "            points_to_check['east'], points_to_check['south'], points_to_check['west']\n",
    "        ]\n",
    "        locations_str = \"|\".join([f\"{lt},{ln}\" for lt, ln in locations_list])\n",
    "        url = \"https://maps.googleapis.com/maps/api/elevation/json\"\n",
    "        params = {'locations': locations_str, 'key': GOOGLE_API_KEY}\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data['status'] == 'OK':\n",
    "            results = data['results']\n",
    "            if not results: return pd.Series([0, 0])\n",
    "            center_elevation = results[0]['elevation']\n",
    "            if center_elevation < 0: is_ocean = 1\n",
    "            surrounding_elevations = [res['elevation'] for res in results[1:]]\n",
    "            for elev in surrounding_elevations:\n",
    "                if abs(center_elevation - elev) > SLOPE_THRESHOLD_METERS:\n",
    "                    is_steep_slope = 1\n",
    "                    break\n",
    "        return pd.Series([is_ocean, is_steep_slope])\n",
    "    except Exception as e:\n",
    "        print(f\"  [오류] Elevation API (Row) 실패: {e}. (0, 0) 반환.\")\n",
    "        return pd.Series([0, 0])\n",
    "\n",
    "def get_usgs_fault_counts(row):\n",
    "    horizontal_count, vertical_count = 0, 0\n",
    "    try:\n",
    "        # (수정) 이 지진이 발생한 시점(Year, Month)을 기준으로 '과거 1년'을 검색\n",
    "        year = int(row['Year'])\n",
    "        month = int(row['Month'])\n",
    "\n",
    "        endtime = f\"{year}-{month:02d}-01\"\n",
    "        starttime = f\"{year - 1}-{month:02d}-01\"\n",
    "\n",
    "        lat = row['latitude']\n",
    "        lon = row['longitude']\n",
    "\n",
    "        search_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "        params = {\n",
    "            'format': 'geojson', 'starttime': starttime, 'endtime': endtime,\n",
    "            'latitude': lat, 'longitude': lon, 'maxradiuskm': USGS_RADIUS_KM,\n",
    "            'minmagnitude': USGS_MIN_MAGNITUDE\n",
    "        }\n",
    "        response = requests.get(search_url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        found_quakes = data.get('features', [])\n",
    "        if not found_quakes: return pd.Series([0, 0])\n",
    "\n",
    "        for quake in found_quakes:\n",
    "            detail_url = quake['properties'].get('detail')\n",
    "            if not detail_url: continue\n",
    "            time.sleep(0.1)\n",
    "            detail_response = requests.get(detail_url, timeout=10)\n",
    "            detail_response.raise_for_status()\n",
    "            detail_data = detail_response.json()\n",
    "            products = detail_data['properties'].get('products', {})\n",
    "            rake_value = None\n",
    "            all_products = products.get('moment-tensor', []) + products.get('focal-mechanism', [])\n",
    "            if not all_products: continue\n",
    "            best_product = None\n",
    "            for p in all_products:\n",
    "                if 'gcmt' in p.get('id','').lower(): best_product = p; break\n",
    "            if best_product is None:\n",
    "                for p in all_products:\n",
    "                     if 'us' in p.get('id','').lower() or p.get('code','').lower() == 'us': best_product = p; break\n",
    "            if best_product is None: best_product = all_products[0]\n",
    "            if best_product:\n",
    "                props = best_product.get('properties', {})\n",
    "                rake_str = props.get('nodal-plane-1-rake')\n",
    "                if rake_str is None: rake_str = props.get('rake')\n",
    "                if rake_str is not None: rake_value = float(rake_str)\n",
    "            if rake_value is not None:\n",
    "                if (45 <= rake_value <= 135) or (-135 <= rake_value <= -45):\n",
    "                    vertical_count += 1\n",
    "                else:\n",
    "                    horizontal_count += 1\n",
    "        return pd.Series([horizontal_count, vertical_count])\n",
    "    except Exception as e:\n",
    "        print(f\"  [오류] USGS API (Row) 실패: {e}. (0, 0) 반환.\")\n",
    "        return pd.Series([0, 0])\n",
    "\n",
    "# --- 4. 메인 코드: 20개 \"왕\"급 데이터 처리 ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if GOOGLE_API_KEY == \"AIzaSyCQt1-_LLhTfX_0l6JAZU1WwlZ1ldkTVTw\":\n",
    "        print(\"=\"*50)\n",
    "        print(\"⚠️ 경고: 12번째 줄의 'GOOGLE_API_KEY'를 입력해야 스크립트가 작동합니다.\")\n",
    "        print(\"=\"*50)\n",
    "        exit()\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv('new_king.csv')\n",
    "        print(f\"'new_kings.csv' 파일 (총 {len(df)}행) 처리 시작...\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"오류: 1단계에서 'new_kings.csv' 파일을 먼저 생성해야 합니다.\")\n",
    "        exit()\n",
    "\n",
    "    tqdm.pandas(desc=\"V22 API 처리 중\")\n",
    "\n",
    "    # (수정) 2개의 API 함수를 20개 행에 대해 순차적으로 실행\n",
    "\n",
    "    print(\"-> 1/2: Elevation API (is_ocean, is_steep_slope) 수집 중...\")\n",
    "    df_elevation = df.progress_apply(\n",
    "        lambda row: get_elevation_features(row['latitude'], row['longitude']),\n",
    "        axis=1\n",
    "    )\n",
    "    df_elevation.columns = ['is_ocean', 'is_steep_slope']\n",
    "\n",
    "    print(\"\\n-> 2/2: USGS API (Fault Counts) 수집 중...\")\n",
    "    df_usgs = df.progress_apply(\n",
    "        get_usgs_fault_counts,\n",
    "        axis=1\n",
    "    )\n",
    "    df_usgs.columns = ['horizontal_count_1y_full', 'vertical_count_1y_full']\n",
    "\n",
    "    # 원본(df)과 2개의 API 결과(df_elevation, df_usgs)를 하나로 합침\n",
    "    df_processed = pd.concat([df, df_elevation, df_usgs], axis=1)\n",
    "\n",
    "    # 4. 새 파일로 저장\n",
    "    output_filename = 'new_kings_processed.csv'\n",
    "    df_processed.to_csv(output_filename, index=False)\n",
    "\n",
    "    print(f\"\\n--- 처리 완료! ---\")\n",
    "    print(f\"모든 특성이 추가된 20개의 데이터가 '{output_filename}'에 저장되었습니다.\")\n",
    "    print(df_processed.head())"
   ],
   "id": "2064d40479160ec3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "⚠️ 경고: 12번째 줄의 'GOOGLE_API_KEY'를 입력해야 스크립트가 작동합니다.\n",
      "==================================================\n",
      "'new_kings.csv' 파일 (총 20행) 처리 시작...\n",
      "-> 1/2: Elevation API (is_ocean, is_steep_slope) 수집 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V22 API 처리 중: 100%|██████████| 20/20 [00:10<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> 2/2: USGS API (Fault Counts) 수집 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V22 API 처리 중: 100%|██████████| 20/20 [00:47<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 처리 완료! ---\n",
      "모든 특성이 추가된 20개의 데이터가 'new_kings_processed.csv'에 저장되었습니다.\n",
      "   magnitude  depth  latitude  longitude  Year  Month  tsunami  is_ocean  \\\n",
      "0        9.5   25.0    -38.14     -73.41  1960      5        1         0   \n",
      "1        9.2   25.0     60.91    -147.34  1964      3        1         0   \n",
      "2        9.1   30.0      3.30      95.98  2004     12        1         1   \n",
      "3        9.1   24.0     38.30     142.37  2011      3        1         1   \n",
      "4        9.0   20.0     52.62     159.78  1952     11        1         1   \n",
      "\n",
      "   is_steep_slope  horizontal_count_1y_full  vertical_count_1y_full  \n",
      "0               0                         0                       0  \n",
      "1               1                         0                       0  \n",
      "2               0                         0                       0  \n",
      "3               0                         0                       2  \n",
      "4               0                         0                       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:30:53.309426Z",
     "start_time": "2025-11-01T15:30:53.299155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. 파일 이름 정의 ---\n",
    "file_original_782 = '../3차 전처리 - 단층 이동 데이터/TEST_train_data_final_with_fault_counts_past_year_full_month.csv'\n",
    "file_new_20 = 'new_kings_processed.csv'\n",
    "file_output_802 = 'final_802_data_for_training.csv'\n",
    "\n",
    "# --- 2. 모델 훈련에 필요한 7개의 핵심 열 정의 ---\n",
    "# (이 순서가 모델이 학습한 순서입니다)\n",
    "MODEL_COLUMNS = [\n",
    "    'magnitude',\n",
    "    'depth',\n",
    "    'is_ocean',\n",
    "    'is_steep_slope',\n",
    "    'horizontal_count_1y_full',\n",
    "    'vertical_count_1y_full',\n",
    "    'tsunami' # 타겟 변수\n",
    "]\n",
    "\n",
    "# --- 3. 데이터 로드 및 필터링 ---\n",
    "try:\n",
    "    df_782 = pd.read_csv(file_original_782)\n",
    "    df_20 = pd.read_csv(file_new_20)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"오류: 파일이 없습니다. {e}\")\n",
    "    print(\"1단계('new_kings.csv')와 2단계('process_new_kings.py')를 먼저 실행하세요.\")\n",
    "    exit()\n",
    "\n",
    "# (전처리) 'is_ocean', 'is_steep_slope'이 0/1이 아닐 경우를 대비\n",
    "df_782['is_ocean'] = df_782['is_ocean'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "df_782['is_steep_slope'] = df_782['is_steep_slope'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "# (df_20은 V22에서 이미 0/1로 생성됨)\n",
    "\n",
    "# 7개의 핵심 열만 남기고 나머지(nst, dmin, Year 등)는 모두 버립니다.\n",
    "df_782_clean = df_782[MODEL_COLUMNS]\n",
    "df_20_clean = df_20[MODEL_COLUMNS]\n",
    "\n",
    "# --- 4. 데이터 병합 (Concat) ---\n",
    "df_final_802 = pd.concat([df_782_clean, df_20_clean], ignore_index=True)\n",
    "\n",
    "# --- 5. 최종 저장 ---\n",
    "df_final_802.to_csv(file_output_802, index=False)\n",
    "\n",
    "print(\"--- 병합 완료! ---\")\n",
    "print(f\"원본 데이터 782개 + '왕'급 데이터 20개 = 총 {len(df_final_802)}개\")\n",
    "print(f\"모델 훈련을 위한 최종 파일: '{file_output_802}'\")"
   ],
   "id": "735c836c63057287",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 병합 완료! ---\n",
      "원본 데이터 782개 + '왕'급 데이터 20개 = 총 802개\n",
      "모델 훈련을 위한 최종 파일: 'final_802_data_for_training.csv'\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:52:18.072203Z",
     "start_time": "2025-11-03T07:52:18.055458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# new_king이랑 final_model_data 합치는 코드 대신 Year Month 포함\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. 파일 이름 정의 ---\n",
    "file_original_782 = 'final_model_data_2013_onward.csv'\n",
    "file_new_20 = 'new_kings_processed.csv'\n",
    "file_output_802 = 'final_802_data_for_training_include_Year_Month.csv'\n",
    "\n",
    "# --- 2. 모델 훈련에 필요한 7개의 핵심 열 정의 ---\n",
    "# (이 순서가 모델이 학습한 순서입니다)\n",
    "MODEL_COLUMNS = [\n",
    "    'magnitude',\n",
    "    'depth',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'is_ocean',\n",
    "    'is_steep_slope',\n",
    "    'horizontal_count_1y_full',\n",
    "    'vertical_count_1y_full',\n",
    "    'tsunami' # 타겟 변수\n",
    "]\n",
    "\n",
    "# --- 3. 데이터 로드 및 필터링 ---\n",
    "try:\n",
    "    df_782 = pd.read_csv(file_original_782)\n",
    "    df_20 = pd.read_csv(file_new_20)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"오류: 파일이 없습니다. {e}\")\n",
    "    print(\"1단계('new_kings.csv')와 2단계('process_new_kings.py')를 먼저 실행하세요.\")\n",
    "    exit()\n",
    "\n",
    "# (전처리) 'is_ocean', 'is_steep_slope'이 0/1이 아닐 경우를 대비\n",
    "df_782['is_ocean'] = df_782['is_ocean'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "df_782['is_steep_slope'] = df_782['is_steep_slope'].apply(lambda x: 1 if (x == True or str(x).lower() == 'true') else 0)\n",
    "# (df_20은 V22에서 이미 0/1로 생성됨)\n",
    "\n",
    "# 7개의 핵심 열만 남기고 나머지(nst, dmin, Year 등)는 모두 버립니다.\n",
    "df_782_clean = df_782[MODEL_COLUMNS]\n",
    "df_20_clean = df_20[MODEL_COLUMNS]\n",
    "\n",
    "# --- 4. 데이터 병합 (Concat) ---\n",
    "df_final_802 = pd.concat([df_782_clean, df_20_clean], ignore_index=True)\n",
    "\n",
    "# --- 5. 최종 저장 ---\n",
    "df_final_802.to_csv(file_output_802, index=False)\n",
    "\n",
    "print(\"--- 병합 완료! ---\")\n",
    "print(f\"원본 데이터 782개 + '왕'급 데이터 20개 = 총 {len(df_final_802)}개\")\n",
    "print(f\"모델 훈련을 위한 최종 파일: '{file_output_802}'\")"
   ],
   "id": "7853763e37cfc1eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 병합 완료! ---\n",
      "원본 데이터 782개 + '왕'급 데이터 20개 = 총 438개\n",
      "모델 훈련을 위한 최종 파일: 'final_802_data_for_training_include_Year_Month.csv'\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:21:08.747371Z",
     "start_time": "2025-11-03T07:21:08.732897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# final_model_data에서 2012년 포함 이전 데이터들 삭제 => tsunami가 모두 0임\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# 1. 파일 이름 정의\n",
    "input_file = 'final_model_data.csv'\n",
    "output_file = 'final_model_data_2013_onward.csv' # 저장할 새 파일 이름\n",
    "\n",
    "try:\n",
    "    # 2. CSV 파일 로드\n",
    "    df = pd.read_csv(input_file)\n",
    "    original_count = len(df)\n",
    "\n",
    "    print(f\"'{input_file}' 파일 로드 완료. (총 {original_count} 행)\")\n",
    "\n",
    "    # 3. 데이터 필터링: 'Year' 컬럼이 2013 이상인 행만 선택\n",
    "    #    (2013년 이전 데이터를 삭제합니다)\n",
    "    df_filtered = df[df['Year'] >= 2013].copy()\n",
    "    filtered_count = len(df_filtered)\n",
    "\n",
    "    # 4. 필터링 결과 출력\n",
    "    print(\"\\n--- 필터링 결과 ---\")\n",
    "    print(f\"원본 데이터 행 수: {original_count}\")\n",
    "    print(f\"2013년 이후 데이터 행 수 (유지): {filtered_count}\")\n",
    "    print(f\"삭제된 행 수 (2013년 미만): {original_count - filtered_count}\")\n",
    "\n",
    "    # 5. 필터링된 데이터를 새 CSV 파일로 저장\n",
    "    df_filtered.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n[성공] 필터링된 데이터가 '{output_file}' 파일로 저장되었습니다.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{input_file}' 파일을 찾을 수 없습니다.\", file=sys.stderr)\n",
    "except KeyError:\n",
    "    print(\"오류: 'Year' 컬럼을 찾을 수 없습니다. 컬럼 이름을 확인하세요.\", file=sys.stderr)\n",
    "except Exception as e:\n",
    "    print(f\"데이터 처리 중 알 수 없는 오류 발생: {e}\", file=sys.stderr)"
   ],
   "id": "fc3ce415c4ae83e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'final_model_data.csv' 파일 로드 완료. (총 782 행)\n",
      "\n",
      "--- 필터링 결과 ---\n",
      "원본 데이터 행 수: 782\n",
      "2013년 이후 데이터 행 수 (유지): 418\n",
      "삭제된 행 수 (2013년 미만): 364\n",
      "\n",
      "[성공] 필터링된 데이터가 'final_model_data_2013_onward.csv' 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-03T07:53:17.844633Z",
     "start_time": "2025-11-03T07:53:17.820717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TEST_earthquakes_final_convert_int 데이터와 final_802_data_for_training_include_Year_Month데이터 합치기\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# 1. 파일 이름 정의\n",
    "# 기준이 될 파일 (컬럼 순서 기준)\n",
    "file_train = 'final_802_data_for_training_include_Year_Month.csv'\n",
    "# 합칠 파일 (순서가 바뀔 파일)\n",
    "file_test = 'TEST_earthquakes_final_convert_int.csv'\n",
    "\n",
    "# 최종 저장될 파일\n",
    "output_file = 'combined_sorted_earthquakes.csv'\n",
    "\n",
    "try:\n",
    "    # 2. 두 CSV 파일 로드\n",
    "    df_train = pd.read_csv(file_train)\n",
    "    df_test = pd.read_csv(file_test)\n",
    "\n",
    "    print(f\"'{file_train}' 로드 완료. (총 {len(df_train)} 행)\")\n",
    "    print(f\"'{file_test}' 로드 완료. (총 {len(df_test)} 행)\")\n",
    "\n",
    "    # 3. 'df_train'의 컬럼 순서를 가져옴\n",
    "    target_columns = df_train.columns.tolist()\n",
    "    print(f\"\\n기준 컬럼 순서:\\n{target_columns}\")\n",
    "\n",
    "    # 4. 'df_test'의 컬럼 순서를 'df_train'에 맞게 재정렬\n",
    "    #    'df_train'에 없는 컬럼이 'df_test'에 있다면(예: latitude) 무시됩니다.\n",
    "\n",
    "    # 'target_columns'에 있는 컬럼만 'df_test'에서 선택하여 순서를 맞춤\n",
    "    try:\n",
    "        df_test_reordered = df_test[target_columns]\n",
    "        print(f\"\\n'{file_test}'의 컬럼 순서를 재정렬했습니다.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"[오류] '{file_test}' 파일에 '{e}' 컬럼이 없습니다.\", file=sys.stderr)\n",
    "        print(\"두 파일의 컬럼 구성이 다른지 확인하세요.\", file=sys.stderr)\n",
    "        sys.exit(1) # 오류 발생 시 중단\n",
    "\n",
    "    # 5. 두 데이터프레임 합치기 (Concatenate)\n",
    "    # df_train (438행) + df_test_reordered (661행)\n",
    "    combined_df = pd.concat([df_train, df_test_reordered], ignore_index=True)\n",
    "\n",
    "    total_rows = len(df_train) + len(df_test)\n",
    "    print(f\"\\n두 파일 병합 완료.\")\n",
    "    print(f\"  - 총 합계 행 수: {len(combined_df)} (예상: {total_rows})\")\n",
    "\n",
    "    # 6. 'Year' 컬럼을 기준으로 내림차순 정렬 (최신순)\n",
    "    if 'Year' in combined_df.columns:\n",
    "        combined_df_sorted = combined_df.sort_values(by='Year', ascending=False)\n",
    "        print(\"\\n'Year' 컬럼 기준 내림차순 정렬 완료.\")\n",
    "\n",
    "        # 7. 최종 결과를 새 CSV 파일로 저장\n",
    "        combined_df_sorted.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n[성공] 최종 병합 및 정렬된 파일이 '{output_file}'로 저장되었습니다.\")\n",
    "\n",
    "        print(\"\\n--- 최종 데이터 상위 5행 ---\")\n",
    "        print(combined_df_sorted.head())\n",
    "    else:\n",
    "        print(\"\\n[오류] 'Year' 컬럼이 없어 정렬을 수행할 수 없습니다.\", file=sys.stderr)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[오류] 파일을 찾을 수 없습니다: {e.filename}\", file=sys.stderr)\n",
    "except Exception as e:\n",
    "    print(f\"[오류] 데이터 처리 중 오류 발생: {e}\", file=sys.stderr)"
   ],
   "id": "5d214f813149b9db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'final_802_data_for_training_include_Year_Month.csv' 로드 완료. (총 438 행)\n",
      "'TEST_earthquakes_final_convert_int.csv' 로드 완료. (총 661 행)\n",
      "\n",
      "기준 컬럼 순서:\n",
      "['magnitude', 'depth', 'latitude', 'longitude', 'Year', 'Month', 'is_ocean', 'is_steep_slope', 'horizontal_count_1y_full', 'vertical_count_1y_full', 'tsunami']\n",
      "\n",
      "'TEST_earthquakes_final_convert_int.csv'의 컬럼 순서를 재정렬했습니다.\n",
      "\n",
      "두 파일 병합 완료.\n",
      "  - 총 합계 행 수: 1099 (예상: 1099)\n",
      "\n",
      "'Year' 컬럼 기준 내림차순 정렬 완료.\n",
      "\n",
      "[성공] 최종 병합 및 정렬된 파일이 'combined_sorted_earthquakes.csv'로 저장되었습니다.\n",
      "\n",
      "--- 최종 데이터 상위 5행 ---\n",
      "      magnitude    depth  latitude  longitude  Year  Month  is_ocean  \\\n",
      "1078        5.1   7.1480   41.2186    78.7240  2024      1         0   \n",
      "1079        5.5  10.0000   41.1979    78.6168  2024      1         0   \n",
      "1080        5.4  10.0000   41.3326    78.7686  2024      1         0   \n",
      "634         5.1   6.1279   32.4140  -102.0570  2024      9         0   \n",
      "635         6.1  33.8820   12.9980   -89.5623  2024      8         1   \n",
      "\n",
      "      is_steep_slope  horizontal_count_1y_full  vertical_count_1y_full  \\\n",
      "1078               0                         1                       0   \n",
      "1079               0                         1                       0   \n",
      "1080               1                         1                       0   \n",
      "634                0                         0                       0   \n",
      "635                1                         1                       2   \n",
      "\n",
      "      tsunami  \n",
      "1078        0  \n",
      "1079        0  \n",
      "1080        0  \n",
      "634         0  \n",
      "635         0  \n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
